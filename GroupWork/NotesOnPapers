As we know it, irony is unique to humans. 

Current nlp methods rely too heavily on
shallow, unstructured, syntactic modeling of text to consistently discern ironic intent.


Irony detection requires semantics that cannot be inffered from word counts alone

Definition of Irony: the expression of one's meaning by using language that normally signifies the opposite, typically for humorous or emphatic effect.

Simply put its a when the speaker says the opposite of what they mean

Definition of Sarcasm: the use of irony to mock or convey contempt.

What are the papers exploring? What were the commonalities and differences?
- They all explored sarcasm, common theme is that they all identify sarcasm as a very hard problem but humans do it with ease.
- Children as young as 6 are able to do it (Pexman and Glenwright 2007).
- Accuractely classifying sarcasm could significantly improve models for sentiment analysis(Davidov et al. 2010; Tepperman et al. 2006), (Pang and Lee 2004).
- Example classifying negative or postive reviews, verbal irony is responsible for many of the errors made by state-of-the-art methods for this
task (Carvalho et al. 2009); 
- Furthermore any model hoping to truly understand human text must distingush between
the sincere and ironic texts 
DIFFERENCES:
   
 How did they explore the topic? (i.e. what was the methodology?) How did they differ?
Used machine learning and nlp

- all of the computational works I review, irony detection has been treated
as a particular instance of text classification

Text classification is the problem of inducing a model, or classifier, capable of categoriz-
ing documents (e.g., newspaper articles) into one of k categories or classes

Popular text classification algorithms include Naive Bayes
(Lewis 1998) and Support Vector Machines (SVMs). The latter is widely accepted to be the
‘state-of-the-art’ in terms of text classification (Joachims 1998).


    What did they find and contribute?

To apply an SVM or any other inductive learning algorithm to a corpus, the documents must first be mapped into vector representations. 
These vectors are points in a feature-space. 
The canonical encoding for text is known as
‘bag-of-words’ (BOW), a simple scheme that keeps a count of the total number of times words are used in a text.

The features here are word counts.
Perhaps surprisingly, this simple binary representation for text has proven sufficient even
for seemingly complex classification tasks. An example is SVMs trained with BOW-encoded
movie reviews can distinguish between ‘positive’ and ‘negative’ reviews labeled by
humans with 80–90 % accuracy (see, e.g., Pang and Lee 2004). 

Using this simple technique, you would hope similar results would be achieved in identifying ironic and sarcastic texts given enough training data, but the results reported so far for the task of automated irony detection have not been nearly as good, implying that a more complex representation of texts needs to be carried out in order to distinguish between ironic and sincere statements.


So using BOW as a baseline one approach that used a more complex method that achieved an 80-90% accuracy is as follows: 

First, they identify patterns, also known as template sentences.
These patterns are automatically extracted from online texts, using an algorithm they proposed elsewhere (Davidov and Rappoport 2006). 

Feature vectors were then made which contained indicators of the presence or absence of these patterns in the corresponding input texts.

They also extracted punctuation-based features such as the number of times a comma or fullstop has been used in a sentence. 

Notice that their approach is entirely shallow; they represent texts with features reflecting surface patterns and punctuation alone. 

Nonetheless, they achieved reasonable, if underwhelming, results with this simple representation; 91 % precision at 76 % recall on the Amazon corpus and 72 % precision at 44 % recall on the Twitter dataset. 7 

An interesting note here is that their approach will, by construction, only be able to detect cliche ironies, such as the Grass is always greener and Actions speak louder than words but not situational irony.


    What was good about the papers? What was bad?

Hardest Part about Irony detection

“Donald Trump sure is smart”. It is likely that in this case the speaker means to express the exact opposite of this statement, and the reciever will probably infer this with ease.
The meaning of which is How might Kelly perform this task?
“it’s raining just a bit” in a down-
pour is to mock that very proposition;


Conclusion:
I have argued that the fundamental
issue with the machine learning techniques that have been proposed to date is that they
operate almost exclusively over the space of textual or syntactic features, and these will
never be sufficient to detect all ironic utterances.Indeed, this is necessarily true, given that the exact same sentence can be intended ironically by one speaker and genuinely by another.
It thus follows that a model of the speaker is imperative for automated irony detection.


10-15 mins

slide 1 - intro
slide 2 - content
Slide 3 - Selection and rationale of chosen extra papers
- go through papers - high level 
slide 3 - define many definitions of irony + sarcasm + examples 
slide 4 - What were the papers exploring? 
slide 5 - common themes and topic
- They all explored sarcasm, common theme is that they all identify sarcasm as a very hard problem but humans do it with ease.
Slide 7 - The Bad
Slide 6 - The Positive
Slide 8 - Understanding and description of limitations


Kumon-Nakamura and Glucksberg (1995).
imagine that a very knowledgable student is arrogantly dominating
a classroom discussion when a classmate remarks to him “boy, you sure know a lot”. The
proposition may be literally true, but the remark is interpreted ironically because of inferred


– The speaker has an expectation E at time t 0 .
– This expectation E fails (is violated) at time t 1 .
– Consequently, the speaker is displeased in this violation of their expectation (i.e., the
disconnect between their belief of what would happen and the reality of what took place).
pragmatic insincerity.

