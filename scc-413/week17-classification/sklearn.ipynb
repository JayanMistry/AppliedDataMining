{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "In this lab you will be using [scikit-learn (sklearn)](https://scikit-learn.org/stable/index.html) to classify texts. sci-kit learn provides numerous classification algorithms for use, and other tools for preprocessing, feature selection, and evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using authorship analysis as an example of classification, but the same techniques can be used for various NLP tasks involving classification, e.g. classifying topics, sentiment classification, etc. The features and documents used should be dictated by the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset (Twitter GB celebs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a small dataset of 75 British Twitter \"celebrities\" for demonstration. A larger dataset of US Twitter celebrities is also available, but the increased will mean that extracting features and fitting models will take longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple function for extracting filenames from a provided folder is given below. We pass filenames to sklearn to load the file and extract text, to avoid loading the whole corpus into memory. We can also pass in text directly (the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, splitext, split\n",
    "\n",
    "def list_files(folder):\n",
    "    textfiles = [join(folder, f) for f in listdir(folder) if isfile(join(folder, f)) and f.endswith(\".txt\")]\n",
    "    return textfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by performing binary classification of gender (gender can be non-binary, but we focus on two genders here to simplify matters). The celeb data has been split into female and male folders, which are read in below and saved to the variable X (the standard \"features\" dataset variable). We store labels (classes) separately in the variable y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 75\n",
      "['celebs-gb/female/imogenheap.txt', 'celebs-gb/female/samantharonson.txt', 'celebs-gb/female/VictoriaCoren.txt', 'celebs-gb/female/esmeeworld.txt', 'celebs-gb/female/camanpour.txt', 'celebs-gb/female/noushskaugen.txt', 'celebs-gb/female/elliegoulding.txt', 'celebs-gb/female/jk_rowling.txt', 'celebs-gb/female/Fearnecotton.txt', 'celebs-gb/female/julianperretta.txt']\n"
     ]
    }
   ],
   "source": [
    "f_files = list_files(\"celebs-gb/female\")\n",
    "m_files = list_files(\"celebs-gb/male\")\n",
    "X = f_files + m_files #X is usually used to denote the dataset to be trained and tested on, i.e. the features (or where features are extracted from)\n",
    "y = [\"female\"] * len(f_files) + [\"male\"] * len(m_files) #y is usually used to store the labels/classes. Here we simply repeat female for how many female users we have, and then the same for male. Obviously X and y must be in same order.\n",
    "\n",
    "print(len(X), len(y))\n",
    "print(X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X and y should be the same length, with elements in the lists corresponding to eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 50\n"
     ]
    }
   ],
   "source": [
    "print(y.count(\"female\"), y.count(\"male\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the dataset is biased to males (2:1). The US dataset has the same bias (2.4:1). This will generally mean that males are predicted more accurately (more training data), and we should consider that a classifier which predicts everybody as male would achieve a raw accuracy of 66.7%. One could undersample the data (e.g. choose a random subset of 25 males), but this would reduce the amount of training data available, or even oversample. This won't be covered here, but feel free to experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to train our model on some data, and test/evaluate on a separate set (later we will see evaluation through cross-validation). sklearn provides functionality for this with [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "\n",
    "We provide the full dataset/corpus (X), the labels (y), and how large the test portion should be. Here we split the data 4 parts training, 1 part test. We provide a random_state so the split is consistent on each run. We stratify the splits (with the labels list (y)) to maintain the same proportions/bias. Returned are X and y split into train and test lists (or the same type as the input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 15\n",
      "60 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['celebs-gb/male/Rickontour.txt',\n",
       " 'celebs-gb/female/VictoriaCoren.txt',\n",
       " 'celebs-gb/male/GordonRamsay.txt',\n",
       " 'celebs-gb/female/jem.txt',\n",
       " 'celebs-gb/male/charltonbrooker.txt',\n",
       " 'celebs-gb/male/DuncanBannatyne.txt',\n",
       " 'celebs-gb/male/simonpegg.txt',\n",
       " 'celebs-gb/male/robertpopper.txt',\n",
       " 'celebs-gb/male/RealDMitchell.txt',\n",
       " 'celebs-gb/male/IanJamesPoulter.txt',\n",
       " 'celebs-gb/male/itspetergabriel.txt',\n",
       " 'celebs-gb/male/AndersFoghR.txt',\n",
       " 'celebs-gb/female/StellaMcCartney.txt',\n",
       " 'celebs-gb/male/serafinowicz.txt',\n",
       " 'celebs-gb/male/Jonnyboy77.txt',\n",
       " 'celebs-gb/male/AndreWinner.txt',\n",
       " 'celebs-gb/male/rustyrockets.txt',\n",
       " 'celebs-gb/male/stephenfry.txt',\n",
       " 'celebs-gb/female/SarahBrownUK.txt',\n",
       " 'celebs-gb/female/KellyOsbourne.txt',\n",
       " 'celebs-gb/male/joemcelderry91.txt',\n",
       " 'celebs-gb/male/bishter.txt',\n",
       " 'celebs-gb/female/samantharonson.txt',\n",
       " 'celebs-gb/male/andrerieu.txt',\n",
       " 'celebs-gb/male/amirkingkhan.txt',\n",
       " 'celebs-gb/female/camanpour.txt',\n",
       " 'celebs-gb/female/ElizabethHurley.txt',\n",
       " 'celebs-gb/male/Richard_H.txt',\n",
       " 'celebs-gb/female/jk_rowling.txt',\n",
       " 'celebs-gb/female/julianperretta.txt',\n",
       " 'celebs-gb/male/richardbranson.txt',\n",
       " 'celebs-gb/male/Gerard_McCarthy.txt',\n",
       " 'celebs-gb/male/joebrooksmusic.txt',\n",
       " 'celebs-gb/male/Robin_Leach.txt',\n",
       " 'celebs-gb/male/PaulMcCartney.txt',\n",
       " 'celebs-gb/female/ariannahuff.txt',\n",
       " 'celebs-gb/male/robbiewilliams.txt',\n",
       " 'celebs-gb/female/Fearnecotton.txt',\n",
       " 'celebs-gb/male/OzzyOsbourne.txt',\n",
       " 'celebs-gb/male/bobbyllew.txt',\n",
       " 'celebs-gb/male/charlesarthur.txt',\n",
       " 'celebs-gb/male/mrjakehumphrey.txt',\n",
       " 'celebs-gb/female/Marsha_Thomason.txt',\n",
       " 'celebs-gb/male/eddieizzard.txt',\n",
       " 'celebs-gb/male/nickjfrost.txt',\n",
       " 'celebs-gb/male/stuholden.txt',\n",
       " 'celebs-gb/male/yannicklawry.txt',\n",
       " 'celebs-gb/female/katenash.txt',\n",
       " 'celebs-gb/male/neilhimself.txt',\n",
       " 'celebs-gb/male/PaoloNutini.txt',\n",
       " 'celebs-gb/female/djrap.txt',\n",
       " 'celebs-gb/female/AmandaHolden.txt',\n",
       " 'celebs-gb/female/lilyallen.txt',\n",
       " 'celebs-gb/male/jaysean.txt',\n",
       " 'celebs-gb/male/jamiecullum.txt',\n",
       " 'celebs-gb/female/Beverleyknight.txt',\n",
       " 'celebs-gb/male/CraigyFerg.txt',\n",
       " 'celebs-gb/male/TimWestwood.txt',\n",
       " 'celebs-gb/female/elliegoulding.txt',\n",
       " 'celebs-gb/female/noushskaugen.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state = 0, stratify=y)\n",
    "print(len(X_train), len(X_test))\n",
    "print(len(y_train), len(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train and y_train are the same length (and corresponding order is maintained), likewise for X_test and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 40\n",
      "5 10\n"
     ]
    }
   ],
   "source": [
    "print(y_train.count(\"female\"), y_train.count(\"male\"))\n",
    "print(y_test.count(\"female\"), y_test.count(\"male\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same proportions are maintained with stratify. Try setting stratify to None, and see the impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have some training and test data, we can start training a model. We have files of texts, from which we need to extract features (the subject of previous labs) to form vectors that can be used in a classifier. sklearn provides some functionality for extracting text features (vectorization), in the form of [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(input='filename', analyzer='word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of the `CountVectorizer` with default settings, except for specifying the input type as `filename` meaning the files are read in and the text extracted. The default is to extract \"words\" from the text, meaning the text is tokenised and the words found counted (raw frequencies are returned).\n",
    "\n",
    "You can try the `char` `analyzer`, which counts characters (codepoints actually, see previous lab), or `char_wb`, which counts characters within words. You can also use ngrams for both words and chars with `ngram_range`. You can also remove stopwords, and several other options.\n",
    "\n",
    "To actually extract features (vectorize), we call `fit_transform` which both creates the vocabulary from the training data (fit), and creates a vector for each training instance (document), which in this case will be the counts for each word in the vocabulary. Because no restriction has been set on the vocabulary, every word type found in the training set will be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we examine this vector, we can see it is large, with 60 rows (for the 60 training instances/documents), and over 170,000 features (the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<60x182467 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 391532 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized.shape\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the features (words) in the vocabulary with `get_feature_names` from the vectorizer (every 5000th feature is shown below). You can see some of the noise in the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '2scip7mvps',\n",
       " '5ztal4vmao',\n",
       " '9frpwdps7r',\n",
       " 'allnock',\n",
       " 'b2yypyxczr',\n",
       " 'bluestr68838024',\n",
       " 'capitals',\n",
       " 'colnemuni',\n",
       " 'darrenhayman',\n",
       " 'dqb1udlfq2',\n",
       " 'endhomelessness',\n",
       " 'fickle',\n",
       " 'gb_shorttrack',\n",
       " 'gy0p2qfh7p',\n",
       " 'hovering',\n",
       " 'intergroup',\n",
       " 'jk1koqkfst',\n",
       " 'kevinkingradio',\n",
       " 'left',\n",
       " 'm6rzzogjdz',\n",
       " 'ministyleblog',\n",
       " 'natcafreitas',\n",
       " 'o5ii4sbhzq',\n",
       " 'palladium',\n",
       " 'pqcqto66yu',\n",
       " 'qw6prq5aoc',\n",
       " 'rjj9unrgjh',\n",
       " 'scartip',\n",
       " 'so0nwfgeg5',\n",
       " 'swipe',\n",
       " 'tidug7mvom',\n",
       " 'uci2hndfoq',\n",
       " 'vhzz7bk6mm',\n",
       " 'whitman',\n",
       " 'xqeiecao2o',\n",
       " 'zcocuzylfr']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[::5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have features, we can train a classifier. We use standard [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) with default settings (except a `random_state` to get consistent results each run). Any classifier could be used here, and you are encouraged to try different classifiers, [many are available](https://scikit-learn.org/stable/supervised_learning.html).\n",
    "\n",
    "Once we have created the classifier, we run `fit`, providing the training vectors and training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(solver='liblinear', random_state=0)\n",
    "clf.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a trained model which we can use to make predictions, i.e. on our test set.\n",
    "\n",
    "First though, we need to convert our test data into the same form as the train set (same features). To do this, we use the vectorizer and call `transform`. What would happen if we instead used `fit_transform`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<15x182467 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 52691 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "X_test_vectorized.shape\n",
    "X_test_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a list of predictions by passing the vectorized test set to the classifier's predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['female' 'male' 'female' 'female' 'male' 'female' 'male' 'male' 'male'\n",
      " 'female' 'female' 'male' 'female' 'female' 'female']\n",
      "['male', 'male', 'male', 'male', 'female', 'female', 'male', 'male', 'male', 'male', 'male', 'male', 'female', 'female', 'female']\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(X_test_vectorized)\n",
    "print(predictions)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate how well the classifier is predicting, we compare to the actual labels (y_test). There are [various metrics available for evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics), a few are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.44      0.80      0.57         5\n",
      "        male       0.83      0.50      0.62        10\n",
      "\n",
      "   micro avg       0.60      0.60      0.60        15\n",
      "   macro avg       0.64      0.65      0.60        15\n",
      "weighted avg       0.70      0.60      0.61        15\n",
      "\n",
      "[[4 1]\n",
      " [5 5]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that performance isn't great, with accuracy equivalent to just predicting male in each case. Though we can see that in fact male and female predictions are both wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows actual against predicted, with actuals as rows, and predicted as columns. Order is the order of the classes in the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So:\n",
    "- top left is the number of actual females predicted as female\n",
    "- top right is the number of actual females predicted as male\n",
    "- bottom left is the number of actual males predicted as female\n",
    "- bottom right is the number of actual males predicted as male.\n",
    "\n",
    "For binary classification, these are actually true positives (tp), false positives (fp), true negatives (tn), and false negatives (fn):\n",
    "\n",
    "`[[tn fp]\n",
    "  [fn tp]]`\n",
    "  \n",
    "The classes (only due to ordering) are 0 (negative): 'female' and 1 (positive): 'male'. Hence a male predicted as male is considered a 'true positive', and a female predicted as female is considered as 'true negatives'. But these labels are meaningless when we are not classifying successes or hits (e.g. for information retrieval).\n",
    "\n",
    "A nice way to view a confusion matrix is with a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def confusion_matrix_heatmap(cm, index):\n",
    "    cmdf = pd.DataFrame(cm, index = index, columns=index)\n",
    "    dims = (5, 5)\n",
    "    fig, ax = plt.subplots(figsize=dims)\n",
    "    sns.heatmap(cmdf, annot=True, cmap=\"coolwarm\", center=0)\n",
    "    ax.set_ylabel('Actual')    \n",
    "    ax.set_xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_heatmap(confusion_matrix(y_test,predictions), clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view which instances were predicted correctly and incorrectly as follows ([`zip`](https://docs.python.org/3.7/library/functions.html#zip) aggregates lists):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(zip(X_test,y_test,predictions,y_test==predictions)), columns=[\"Instance\", \"Actual\", \"Predicted\",\"Correct\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Steps for vectorization, preprocessing, normalising, feature selection, classification, and more can be combined with an sklearn [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). They conveniently combine steps to transform and fit training data. The same pipeline can then be used to transform and predict test data. They also make cross-validation much simpler, by running train/test on each fold by processing the pipeline in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word')),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we create a pipeline exactly as we have used above. To perform all transforms (before classifier), and do final training (fit), we simply use `fit`, providing the training data as below. `transform_fit` is called on all steps before final step has `fit` called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a trained model. It is important to note, the Pipeline provides all functions from the last step, which in this case is a classifier. We can get predictions by simply calling `predict`. This performs all transforms on the (already fitted) steps before the classifier, and then finally predict on the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are exactly the same as above, as we've used the same steps. We can edit the pipeline parameters, or even replace whole steps. To reference a step, we use `step__param`. For example, to add a `max_features` to the vectorizer (to limit the number of features to the top n):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_params(vectorizer__max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the classifier we use the same process, just referencing the named step for the classifier (`clf` here). Below we change to a [Multinomial Naive Bayes classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model.set_params(clf=MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "So far we have been using a single train/test split. What if our test set happens to be particularly easy or hard to predict? What if our training set contains noisier data than average. As discussed in the lecture, cross validation helps here by performing multiple evaluations, splitting the data into different train/test splits.\n",
    "\n",
    "With pipleines and skklearn, performing cross validation is straightforward, and we can be ensured that the entire pipeline is fit and transformed for each fold individually. [Various methods are available for cross validation](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection), we use [`cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) (to return multiple metrics) with [`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold), with 5 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "cv_scores = cross_validate(model, X, y, \n",
    "                           cv=StratifiedKFold(n_splits=5, random_state=0), \n",
    "                           return_train_score=False, \n",
    "                           scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cv_scores_summary(name, scores):\n",
    "    print(\"{}: mean = {:.2f}%, sd = {:.2f}%, min = {:.2f}, max = {:.2f}\".format(name, scores.mean()*100, scores.std()*100, scores.min()*100, scores.max()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are returned for all folds, and by looking at the mean, standard deviation, and range of scores, one can observe the classifier performance more thoroughly. It would appear there is some variance between folds, and the original train/test split was quite lucky in terms of achieving the maximum accuracy observed under cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Normalization\n",
    "So far we have been using raw frequencies in our model. Often we want to standardize, scale or normalize features. [Various options are available](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing). A few options are demonstrated below.\n",
    "\n",
    "For 'standardization', i.e. to zero mean and unit variance, you can use [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler).\n",
    "\n",
    "For 'normalization', i.e. setting each instance's feature vector's norm (l1 or l2) to 1, you can use [`Normalizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer). Using l1 norm, it is possible to get something equivalent to relative frequencies of words (i.e. dividing by total frequency), but this is only the case if every word is included as a feature (which isn't normally the case).\n",
    "\n",
    "A more common normalization for text data is the already discussed TF-IDF. sklearn's [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) can be utilised to calculate TF-IDF scores, though the implementation is different to that normally used, and includes l2 normaalization after TF-IDF is calculated by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word', max_features=1000)),\n",
    "    ('norm', TfidfTransformer(norm=None)),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_validate(model, X, y, cv=StratifiedKFold(n_splits=5, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, another common normalization technique is to binarize features (i.e. scale to 0 for not present or 1 for present at any frequency), this is useful when documents are shorts (e.g. individual Tweets). This is performed with [`Binarizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html#sklearn.preprocessing.Binarizer). Here we perform binarization before TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word', max_features=1000)),\n",
    "    ('norm', Binarizer()),\n",
    "    ('norm2', TfidfTransformer(norm=None)),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_validate(model, X, y, cv=StratifiedKFold(n_splits=5, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "When the number of features is high, one option available is some form of dimensionality reduction or feature selection, e.g. to remove features with low variance across instances/documents, to select features with significant differences between classes, or features that have high weights/coefficients in other models. [Various methods are available for this](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection). An example below selects 1000 features with the highest values for the [chi-squared test](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word')),\n",
    "    ('selector', SelectKBest(chi2, k=1000)),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_validate(model, X, y, cv=StratifiedKFold(n_splits=5, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to view the selected features, the code below will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(input='filename', analyzer='word')\n",
    "selector = SelectKBest(chi2, k=100)\n",
    "feats = vectorizer.fit_transform(X_train)\n",
    "filtered = selector.fit_transform(feats, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "cols = selector.get_support()\n",
    "names = vectorizer.get_feature_names()\n",
    "\n",
    "print(list(compress(names,cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter optimizers with GridSearch\n",
    "\n",
    "We have played around with different options available in sklearn, but for actual experiments you will want to be more structured and scientific. [Various optimizers are available](https://scikit-learn.org/stable/modules/classes.html#hyper-parameter-optimizers). [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) allows for an exhaustive search across a range of listed parameter values, and then runs cross validation for every model built (for every combination of parameters. With a large number of parameters, the search will take some time run due to the number of models that need to be fit and tested.\n",
    "\n",
    "An example is given below with top_k from chi squared feature selection set to different values, and two different classifiers (Naive Bayes and Logistic Regression). This could easily be extended to introduce new paramaters and steps in the pipeline.\n",
    "\n",
    "Note, we are performing our cross-validation over just the training data (X_train). X_test is reserved for testing the best model. By supplying a scoring parameter to `refit`, the best performing parameters are selected from the grid search, and a fitted model across the entire provided instances (here `X_train`) is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word')),\n",
    "    ('selector', SelectKBest(score_func = chi2)),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])\n",
    "\n",
    "search = GridSearchCV(model, cv=StratifiedKFold(n_splits=5, random_state=0), \n",
    "                      return_train_score=False, \n",
    "                      scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'],\n",
    "                      refit = 'f1_weighted',\n",
    "                      param_grid={\n",
    "                          'selector__k': [10, 50, 100, 250, 500, 1000],\n",
    "                          'clf': [MultinomialNB(), LogisticRegression(solver='liblinear', random_state=0)],\n",
    "                      })\n",
    "\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = search.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "confusion_matrix_heatmap(confusion_matrix(y_test,predictions), search.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customising preprocessing and feature extraction\n",
    "So far we have been using sklearn's own tokenisation and basic bag-of-words. What about all that you've learnt for preprocessing, tokenisation, and feature extraction in previous labs? Surely we can do better than BoW with broken tokenisation!\n",
    "\n",
    "Included below are some basic methods for preprocessing and tokenisation from last week's lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "hashtag_re = re.compile(r\"#\\w+\")\n",
    "mention_re = re.compile(r\"@\\w+\")\n",
    "url_re = re.compile(r\"(?:https?://)?(?:[-\\w]+\\.)+[a-zA-Z]{2,9}[-\\w/#~:;.?+=&%@~]*\")\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    p_text = hashtag_re.sub(\"[hashtag]\",text)\n",
    "    p_text = mention_re.sub(\"[mention]\",p_text)\n",
    "    p_text = url_re.sub(\"[url]\",p_text)\n",
    "    p_text = ftfy.fix_text(p_text)\n",
    "    return p_text\n",
    "\n",
    "tokenise_re = re.compile(r\"(\\[[^\\]]+\\]|[-'\\w]+|[^\\s\\w\\[']+)\") #([]|words|other non-space)\n",
    "def custom_tokenise(text):\n",
    "    return tokenise_re.findall(text.lower())\n",
    "\n",
    "def nltk_twitter_tokenise(text):\n",
    "    twtok = nltk.tokenize.TweetTokenizer()\n",
    "    return twtok.tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can utilise these methods with `CountVectorizer` by simply passing the callable functions for `preprocessor` and `tokenizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(input='filename', analyzer='word', tokenizer=custom_tokenise, preprocessor=preprocess)\n",
    "feats = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the vocabulary has reduced by more than half. Why is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()[::2500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at the features also indicates that they are much more sensible as \"words\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word',tokenizer=custom_tokenise, preprocessor=preprocess)),\n",
    "    ('selector', SelectKBest(chi2, k=1000)),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_validate(model, X, y, cv=StratifiedKFold(n_splits=5, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "This week's work is to simply build the best gender classifier you can, utilising what has been shown above. Using the gb data is fine, but you can try the usa data if you wish. Use `GridSearchCV` to evaluate different models, and fit the best performing model & parameters. Report the final results on the the test set. An unseen test set will be provided for Week 18's lab which you will be expected to demonstrate performance on. Example parameters you could change:\n",
    "\n",
    "- Classifiers\n",
    "- Classifier hyper-parameters\n",
    "- Feature selections\n",
    "- Normalization / standardization / binarization\n",
    "- Feature size cut-offs\n",
    "- Tokenisers\n",
    "- Preprocessing steps\n",
    "- Word / Characters\n",
    "- Ngram range\n",
    "\n",
    "Marks will be given as follows.\n",
    "\n",
    "- 3 marks will be given for a good range of the above (> 3).\n",
    "- 2 marks will be given for a completed analysis.\n",
    "- 1 mark will be given for a full attempt.\n",
    "\n",
    "This exercise must be demonstrated by Week 18's lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still to come\n",
    "\n",
    "A further lab will be available for next week with custom analysers, pre-processed features, feature analysis, and feature unions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init vectorizer\n",
      "reading files\n",
      "train test split\n",
      "clf1 pipeline finished\n",
      "clf1 gridsearch finished\n",
      "clf2 pipeline finished\n",
      "clf2 gridsearch finished\n",
      "clf3 pipeline finished\n",
      "clf3 gridsearch finished\n",
      "clf4 pipeline finished\n",
      "clf4 gridsearch finished\n",
      "clf5 pipeline finished\n",
      "fitting clf\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from os import listdir\n",
    "from os.path import isfile, join, splitext, split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import ftfy\n",
    "import nltk\n",
    "import re\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostRegressor, VotingClassifier, RandomForestClassifier\n",
    "%matplotlib inline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import svm\n",
    "\n",
    "tokenise_re = re.compile(r\"(\\[[^\\]]+\\]|[-'\\w]+|[^\\s\\w\\[']+)\") #([]|words|other non-space)\n",
    "hashtag_re = re.compile(r\"#\\w+\")\n",
    "mention_re = re.compile(r\"@\\w+\")\n",
    "url_re = re.compile(r\"(?:https?://)?(?:[-\\w]+\\.)+[a-zA-Z]{2,9}[-\\w/#~:;.?+=&%@~]*\")\n",
    "\n",
    "def list_files(folder):\n",
    "    textfiles = [join(folder, f) for f in listdir(folder) if isfile(join(folder, f)) and f.endswith(\".txt\")]\n",
    "    return textfiles\n",
    "\n",
    "def custom_tokenise(text):\n",
    "    return tokenise_re.findall(text.lower())\n",
    "\n",
    "def preprocess(text):\n",
    "    p_text = hashtag_re.sub(\"[hashtag]\",text)\n",
    "    p_text = mention_re.sub(\"[mention]\",p_text)\n",
    "    p_text = url_re.sub(\"[url]\",p_text)\n",
    "    p_text = ftfy.fix_text(p_text)\n",
    "    return p_text\n",
    "\n",
    "\n",
    "\n",
    "def confusion_matrix_heatmap(cm, index):\n",
    "    cmdf = pd.DataFrame(cm, index = index, columns=index)\n",
    "    dims = (5, 5)\n",
    "    fig, ax = plt.subplots(figsize=dims)\n",
    "    sns.heatmap(cmdf, annot=True, cmap=\"coolwarm\", center=0)\n",
    "    ax.set_ylabel('Actual')    \n",
    "    ax.set_xlabel('Predicted')\n",
    "\n",
    "print('init vectorizer')\n",
    "#vectorizer = CountVectorizer(input='filename', analyzer='word')\n",
    "print('reading files')\n",
    "f_files = list_files(\"celebs-usa/female\")\n",
    "m_files = list_files(\"celebs-usa/male\")\n",
    "\n",
    "gb_f_files = list_files(\"celebs-gb/female\")\n",
    "gb_m_files = list_files(\"celebs-gb/male\")\n",
    "\n",
    "X = f_files + gb_f_files +  m_files + gb_m_files \n",
    "y = [\"female\"] * (len(f_files) + len(gb_f_files)) + [\"male\"] * (len(m_files)+len(gb_m_files)) \n",
    "\n",
    "print('train test split')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state = 0, stratify=y)\n",
    "\n",
    "#X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "\n",
    "clf1 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word',tokenizer=custom_tokenise, preprocessor=preprocess)),\n",
    "    ('selector', SelectKBest(chi2, k=1000)),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])\n",
    "\n",
    "print('clf1 pipeline finished')\n",
    "\n",
    "cf1 = GridSearchCV(clf1, cv=StratifiedKFold(n_splits=5, random_state=0), \n",
    "                      return_train_score=False, \n",
    "                      scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'],\n",
    "                      refit = 'f1_weighted',\n",
    "                      param_grid={\n",
    "                          'selector__k': [10, 50, 100, 250, 500, 1000],\n",
    "                          'clf': [LogisticRegression(solver='liblinear', random_state=0)],\n",
    "                      })\n",
    "\n",
    "print('clf1 gridsearch finished')\n",
    "\n",
    "clf2 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word',tokenizer=custom_tokenise, preprocessor=preprocess)),\n",
    "    ('selector', SelectKBest(chi2, k=1000)),\n",
    "    ('clf', MLPClassifier(random_state=42, hidden_layer_sizes=30, activation='relu')),\n",
    "])\n",
    "\n",
    "print('clf2 pipeline finished')\n",
    "\n",
    "\n",
    "cf2 = GridSearchCV(clf1, cv=StratifiedKFold(n_splits=5, random_state=0), \n",
    "                      return_train_score=False, \n",
    "                      scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'],\n",
    "                      refit = 'f1_weighted',\n",
    "                      param_grid={\n",
    "                          'selector__k': [10, 50, 100, 250, 500, 1000],\n",
    "                          'clf': [LogisticRegression(solver='liblinear', random_state=0)],\n",
    "                           'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,),(25,25,25,25,25) ],\n",
    "                            'activation': ['tanh', 'relu'],\n",
    "                            'solver': ['sgd', 'adam'],\n",
    "                            'alpha': [0.0001, 0.05],\n",
    "                            'learning_rate': ['constant','adaptive'],\n",
    "                      })\n",
    "\n",
    "print('clf2 gridsearch finished')\n",
    "\n",
    "\n",
    "clf3 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word',tokenizer=custom_tokenise, preprocessor=preprocess)),\n",
    "    ('selector', SelectKBest(chi2, k=1000)),\n",
    "    ('clf', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "print('clf3 pipeline finished')\n",
    "\n",
    "cf3 = GridSearchCV(clf1, cv=StratifiedKFold(n_splits=5, random_state=0), \n",
    "                      return_train_score=False, \n",
    "                      scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'],\n",
    "                      refit = 'f1_weighted',\n",
    "                      param_grid={\n",
    "                          'selector__k': [10, 50, 100, 250, 500, 1000],\n",
    "                          'clf': [LogisticRegression(solver='liblinear', random_state=0)],\n",
    "                      })\n",
    "\n",
    "print('clf3 gridsearch finished')\n",
    "\n",
    "\n",
    "clf4 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word',tokenizer=custom_tokenise, preprocessor=preprocess)),\n",
    "    ('selector', SelectKBest(chi2, k=1000)),\n",
    "    ('clf', RandomForestClassifier(random_state=1)),\n",
    "])\n",
    "\n",
    "print('clf4 pipeline finished')\n",
    "\n",
    "\n",
    "cf4 = GridSearchCV(clf1, cv=StratifiedKFold(n_splits=5, random_state=0), \n",
    "                      return_train_score=False, \n",
    "                      scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'],\n",
    "                      refit = 'f1_weighted',\n",
    "                      param_grid={\n",
    "                         'dndsjknsenfd': [10, 50, 100, 250, 500, 1000],\n",
    "                          'clf': [LogisticRegression(solver='liblinear', random_state=0)],\n",
    "                      })\n",
    "\n",
    "print('clf4 gridsearch finished')\n",
    "\n",
    "'''\n",
    "clf5 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(input='filename', analyzer='word',tokenizer=custom_tokenise, preprocessor=preprocess)),\n",
    "    ('selector', SelectKBest(chi2, k=1000)),\n",
    "    ('clf', svm.SVC(gamma='scale')),\n",
    "])\n",
    "'''\n",
    "print('clf5 pipeline finished')\n",
    "\n",
    "\n",
    "VotingEnsemble = VotingClassifier(estimators=[('lr', clf1), ('mlp', clf2), ('tree', clf3), ('randfor', clf4)], voting='hard')\n",
    "\n",
    "print('fitting clf')\n",
    "\n",
    "\n",
    "VotingEnsemble.fit(X_train, y_train)\n",
    "\n",
    "print('trained')\n",
    "\n",
    "\n",
    "joblib.dump(VotingEnsemble, 'model.joblib')\n",
    "\n",
    "\n",
    "VotingEnsemble = joblib.load('model.joblib')\n",
    "\n",
    "\n",
    "\n",
    "predictions = VotingEnsemble.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "confusion_matrix_heatmap(confusion_matrix(y_test,predictions), search.classes_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "search = GridSearchCV(model, cv=StratifiedKFold(n_splits=5, random_state=0), \n",
    "                      return_train_score=False, \n",
    "                      scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'],\n",
    "                      refit = 'f1_weighted',\n",
    "                      param_grid={\n",
    "                          'selector__k': [10, 50, 100, 250, 500, 1000],\n",
    "                          'clf': [LogisticRegression(solver='liblinear', random_state=0)],\n",
    "                      })\n",
    "\n",
    "'''\n",
    "\n",
    "#ABC = AdaBoostRegressor(base_estimator = model, n_estimators = 5)\n",
    "\n",
    "# Fit to training data\n",
    "\n",
    "#ABC.fit(X_train, y_train)\n",
    "\n",
    "# Score on testing data\n",
    "#acc = ABC.score(X_test, y_test)\n",
    "#boosted_model_score = boosted_model.score(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "print('fitting')\n",
    "search.fit(X_train, y_train)\n",
    "print('scoring')\n",
    "predictions = search.predict(X_test)\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"n_estimators\": [1, 2,3]\n",
    "             }\n",
    "\n",
    "\n",
    "DTC = DecisionTreeClassifier(random_state = 11, max_features = \"auto\", class_weight = \"auto\", max_depth = 3)\n",
    "\n",
    "ABC = AdaBoostClassifier(base_estimator = model)\n",
    "\n",
    "# run grid search\n",
    "grid_search_ABC =  GridSearchCV(model, cv=StratifiedKFold(n_splits=5, random_state=0), \n",
    "                      return_train_score=False, \n",
    "                      scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'],\n",
    "                      refit = 'f1_weighted',\n",
    "                      param_grid={\n",
    "                          'selector__k': [10, 50, 100, 250, 500, 1000],\n",
    "                          'clf': [LogisticRegression(solver='liblinear', random_state=0)],\n",
    "                      })\n",
    "\n",
    "grid_search_ABC.fit(X_train,y_train)\n",
    "predictions = grid_search_ABC.predict(X_test)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "abc.ABCMeta"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VotingEnsemble = joblib.load('model.joblib')\n",
    "\n",
    "type(VotingEnsemble)\n",
    "\n",
    "\n",
    "predictions = VotingEnsemble.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "confusion_matrix_heatmap(confusion_matrix(y_test,predictions), search.classes_)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
