{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation\n",
    "\n",
    "In this lab you will be honing your regular expression skills to perform the key task of tokenisation. The aim of tokenisation is to separate the text into meaningful components that are useful for future analysis (e.g. counting or annotating). Often the most logical token is a \"word\" (e.g. for \"bag-of-words\" based methods), but deciding what constitutes a word is not straight-forward. At other times, punctuation should be maintained (e.g. for part of speech annotation). You will see that choosing the wrong tokenisation can impact follow-on analysis of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be trying different tokenisers. Four texts are provided for testing the tokenisers: [tweet.txt](tweet.txt) - a single tweet for testing, [mumsnet.txt](mumsnet.txt) - a collection of mumsnet forum posts as collected in last week's lab, and [mirrormirror.txt](mirrormirror.txt) & [charliex.txt](charliex.txt) - short plot summaries of Star Trek episodes from Wikipedia. You are also welcome to collect and use your own data, utilising techniques from last week's lab. [tweet.txt](tweet.txt) will be used primarily below, but feel free to swap any text throughout to aid your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer, [tokeniser.py](tokeniser.py) provides skeleton code for what is presented below. It simply runs the tokenisation method provided in `tokenise(text)` over the text file provided on the command line, printing out the tokens 1 per line (as is standard for tokenisation), and a total count of tokens found. To run the tokeniser over a file simply run:\n",
    "\n",
    "```\n",
    "$ python3 tokeniser.py infile.txt\n",
    "```\n",
    "\n",
    "You can redirect the printed output to a file if you wish, simply add `> outfile.txt` to the end of the command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions will be used in this lab to define regularly used code.\n",
    "\n",
    "First to open a file, tokenise it with a provided tokeniser method, and return a list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes file and tokeniser function, reads line by line, and returns list of tokens.\n",
    "def tokenise_file(textfile, tokenise):\n",
    "    with open(textfile, encoding=\"utf-8\") as f:\n",
    "        tokens = []\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line_tokens = tokenise(line.strip())\n",
    "            tokens.extend(line_tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print a list of tokens and the number of tokens present, and to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens(tokens):\n",
    "    for token in tokens: #iterate tokens and print one per line.\n",
    "        print(token)\n",
    "    print(f\"Total: {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokens(tokens, outfile):\n",
    "    with open(outfile, 'w', encoding=\"utf-8\") as f:\n",
    "        for token in tokens: #iterate tokens and output to file.\n",
    "            f.write(token + '\\n')\n",
    "        f.write(f\"Total: {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we'll definitely need the regular expressions package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reference\n",
    "with open(\"tweet.txt\", encoding=\"utf-8\") as f:\n",
    "    tweet = f.read()\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest method of tokenisation is to just split the text on whitespace. The tokeniser below does just this, using Python's split function, simply on a space character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenise(text):\n",
    "    return text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The whitespace_tokenise function has been defined, so can be passed as a callable object to the tokenise_file function.\n",
    "tokens = tokenise_file(\"tweet.txt\", whitespace_tokenise)\n",
    "print(tweet)\n",
    "print()\n",
    "print_tokens(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the output, what potential issues can you observe? Are any characters missing from the original input? Can you improve it?\n",
    "\n",
    "- **Optional:** Can you think of a better white space tokeniser, e.g. that splits on multiple spaces, or other white space characters using a regular expression?\n",
    "\n",
    "- **Optional:** Can you come up with an alternative than using split, and instead using findall with a regular expression to do the same? (See simple tokeniser below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can try a very simple tokeniser that instead of finding the space between tokens, looks for patterns that match words. We use a basic regular expression for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_match_tokenise(text):\n",
    "    p = re.compile(r\"[a-zA-Z]+\")\n",
    "    return p.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenise_file(\"tweet.txt\", simple_match_tokenise)\n",
    "print(tweet)\n",
    "print()\n",
    "print_tokens(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, observe the output. What potential issues do you see? Why do you have more tokens? Are any characters missing from the original input?\n",
    "- **Optional**: Can you invert the function to use split instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main task of this lab is to write your own tokeniser to list tokens useful for different purposes and different texts. You have a template for this in `custom_tokenise`. Here, a list of regular expression patterns are used to search for different types of tokens in turn, utilising *alternation* in one large compiled regular expression. You are provided with patterns for URLs, and simple words. Try this out on the tweet, the URLs should be tokenised separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenise(text):\n",
    "    URL = '(?:https?://)?(?:[-\\w]+\\.)+[a-zA-Z]{2,9}[-\\w/#~:;.?+=&%@~]*' #this is one possible URL pattern, more complicated patterns that catch different URLs are possible.\n",
    "    word = '\\w+'\n",
    "    patterns = (URL, word)\n",
    "    joint_patterns = '|'.join(patterns) #the patterns are split with | for alternation.\n",
    "    p = re.compile(r'(?:{})'.format(joint_patterns)) # format is used to build the pattern, surrounding with (?:...) for non-captured grouping for alternation.\n",
    "    return p.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenise_file(\"tweet.txt\", custom_tokenise)\n",
    "print(tweet)\n",
    "print()\n",
    "print_tokens(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A point to be aware of here is the ordering of the pattern sequence. Regular expressions in Python (and most other languages) process alternation options in order (left to right). This means a longer match (i.e. greedy) may be ignored if an earlier option leads to a successful match (regular expression finds will not overlap). It is therefore important to consider the order of the regular expressions, as a more general pattern may match some text before a more specific pattern has chance to see it. You can see this in action by swapping the URL and word patterns (`patterns = (word,URL)`) and tokenising the tweet again, now the URL will not be tokenised separately as the word pattern hits first and consumes the start of the URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to aid understanding, a tokeniser is provided below specialised for the tokenising the [mirrormirror.txt](mirrormirror.txt). Here the following tokenisation rules have been applied:\n",
    "- all punctuation is separated as individual tokens\n",
    "- titles and initials are single tokens (e.g. `Dr.` and `T.`)\n",
    "- hyphenated words are a single token (e.g. `mirror-universe`\n",
    "- cases of possessive s (`'s`) are separated, e.g. `Kirk` & `'s` are separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenise_mirror(text):\n",
    "    title = \"[A-Z][a-z]?\\.\"\n",
    "    word = \"[-\\w]+\"\n",
    "    apos = \"\\'[a-z]*\"\n",
    "    other_chars = \"[^\\w\\s]\"\n",
    "    leftover = \"\\S+\" #Having a final catch all of \"non-white-space\" will pick up anything not explicity looked for earlier.\n",
    "    patterns = (title, apos, other_chars, word, leftover) #leftover not actually needed here, as all caught in other chars.\n",
    "    joint_patterns = '|'.join(patterns)\n",
    "    p = re.compile(r'(?:{})'.format(joint_patterns))\n",
    "    return p.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenise_file(\"mirrormirror.txt\", custom_tokenise_mirror)\n",
    "save_tokens(tokens,\"mirrormirror_tok.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the output in [mirrormirror_tok.txt](mirrormirror_tok.txt). Check you understand which patterns are catching which tokens.\n",
    "\n",
    "Can you edit the tokeniser so that punctuation tokens aren't included, but the tokens above (titles and initials) with punctuation are still present?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced**: Try running your tokeniser on [charliex.txt](charliex.txt) too. This is very similar to mirrormirror.txt, but it also contains contractions (e.g. `don't`). How are these dealt with by the above tokeniser? Ideally, `n't` should be a separate token, to represent `not`. Consider me impressed if you can achieve this in your tokeniser (without breaking other parts), especially if you can do it without adding pre- or post-processing. Let Alistair know if you manage this, or if you would like a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All NLP toolkits, and many NLP tools themselves have built in tokenisers. [NLTK](http://www.nltk.org) is one of the best known ones and has a specialsed tokeniser for online text, and specifically Twitter data. Running the tokenisers is straight-forward, with methods provided below, `nltk_tokenise`: default, and `nltk_twitter_tokenise`: for Twitter. Note the \"Twitter\" tokeniser will be applicable to other online text too, e.g. forum data. Run these tokenisers over the texts and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') #download tokeniser models, will download first time, and find on second run. You can remove this line once installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenise(text):\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_twitter_tokenise(text):\n",
    "    twtok = nltk.tokenize.TweetTokenizer()\n",
    "    return twtok.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenise_file(\"tweet.txt\", nltk_tokenise)\n",
    "print(tweet)\n",
    "print()\n",
    "print_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenise_file(\"tweet.txt\", nltk_twitter_tokenise)\n",
    "print(tweet)\n",
    "print()\n",
    "print_tokens(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is dealt with better by NLTK? Any issues that you were able to overcome with a manual tokeniser?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency analysis\n",
    "Once we have text tokenised we can start to count words, and do some analysis. We will start in earnest with this next week. For now, we will do a simple counting of tokens and producing a frequency list and plot.\n",
    "\n",
    "The `frequency_analysis` method takes a list of tokens (outputted from the various tokenise methods) and counts the frequency of each token. The list of tokens is printed alongside the frequency of each, in descending frequency order. A frequency plot is also produced, with the top 20 tokens as default.\n",
    "\n",
    "Examining the frequency list (and token streams) can help to find common tokenisation issues.\n",
    "\n",
    "You can try this on different texts. Increase the number of items in the plot, given enough text you should start to see a nice [Zipfian curve](https://en.wikipedia.org/wiki/Zipf\\%27s_law). Try `cumulative=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def frequency_analysis(tokens):\n",
    "    freq = nltk.FreqDist(tokens)\n",
    "    for key,val in freq.most_common(20):\n",
    "        print(key,val,sep=\"\\t\")\n",
    "\n",
    "    freq.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenise_file(\"mirrormirror.txt\", custom_tokenise_mirror)\n",
    "frequency_analysis(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Build a tokeniser for the mumsnet forum data ([mumsnet.txt](mumsnet.txt) (feel free to use your own collected data too) by building on the `custom_tokenise` method. Tokenise the text with the following rules:\n",
    "- all punctuation separated as individual tokens, unless sequences of punctuation (e.g. `!!!`), which should be combined to a single token.\n",
    "- URLs, hashtags, and mentions as separate tokens\n",
    "- hyphenated words as a single token\n",
    "- words with apostrophes to mark concatenation and possessive s should be a single token (e.g. `don't`, `I'm` and `1940's`)\n",
    "- **Advanced/Extra**: Emoticons separated as separate tokens (e.g. `:-)`)\n",
    "- **Advanced/Extra**: You will find other sequences in the text that should be single tokens, deal with as many of these as possible for as clean a tokenisation as possible ready for creating a frequency list.\n",
    "\n",
    "You may want to consider pre-processing (i.e. cleaning/normalising) the text to make tokenisation easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenise_forum(text):\n",
    "    URL = '(?:https?://)?(?:[-\\w]+\\.)+[a-zA-Z]{2,9}[-\\w/#~:;.?+=&%@~]*' #this is one possible URL pattern, more complicated patterns that catch different URLs are possible.\n",
    "    word = '\\w+'\n",
    "    patterns = (URL, word)\n",
    "    joint_patterns = '|'.join(patterns) #the patterns are split with | for alternation.\n",
    "    p = re.compile(r'(?:{})'.format(joint_patterns)) # format is used to build the pattern, surrounding with (?:...) for non-captured grouping for alternation.\n",
    "    return p.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenise_file(\"mumsnet.txt\", custom_tokenise_forum)\n",
    "save_tokens(tokens,\"mumsnet_tok.txt\")\n",
    "frequency_analysis(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note any findings or decisions made here:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
