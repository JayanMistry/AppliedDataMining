{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classification\n",
    "\n",
    "Below are imports and helper functions from previous classification labs. The task for this week is to build your own classifier for predicting the sentiment of Tweets. Tweets are provided from [SemEval-2016 Task 4 (Subtask A)](http://alt.qcri.org/semeval2016/task4/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, splitext, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For POS tagger, incase these haven't been previously downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/jay/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('maxent_treebank_pos_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of methods for showing classifier results (from 1st classification lab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cv_scores_summary(name, scores):\n",
    "    print(\"{}: mean = {:.2f}%, sd = {:.2f}%, min = {:.2f}, max = {:.2f}\".format(name, scores.mean()*100, scores.std()*100, scores.min()*100, scores.max()*100))\n",
    "    \n",
    "def confusion_matrix_heatmap(cm, index):\n",
    "    cmdf = pd.DataFrame(cm, index = index, columns=index)\n",
    "    dims = (10, 10)\n",
    "    fig, ax = plt.subplots(figsize=dims)\n",
    "    sns.heatmap(cmdf, annot=True, cmap=\"coolwarm\", center=0)\n",
    "    ax.set_ylabel('Actual')    \n",
    "    ax.set_xlabel('Predicted')\n",
    "    \n",
    "def confusion_matrix_percent_heatmap(cm, index):\n",
    "    cmdf = pd.DataFrame(cm, index = index, columns=index)\n",
    "    percents = cmdf.div(cmdf.sum(axis=1), axis=0)*100\n",
    "    dims = (10, 10)\n",
    "    fig, ax = plt.subplots(figsize=dims)\n",
    "    sns.heatmap(percents, annot=True, cmap=\"coolwarm\", center=0, vmin=0, vmax=100)\n",
    "    ax.set_ylabel('Actual')    \n",
    "    ax.set_xlabel('Predicted')\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_ticks([0, 25, 50, 75, 100])\n",
    "    cbar.set_ticklabels(['0%', '25%', '50%', '75%', '100%'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_re = re.compile(r\"#\\w+\")\n",
    "mention_re = re.compile(r\"@\\w+\")\n",
    "url_re = re.compile(r\"(?:https?://)?(?:[-\\w]+\\.)+[a-zA-Z]{2,9}[-\\w/#~:;.?+=&%@~]*\")\n",
    "\n",
    "def preprocess(text):\n",
    "    p_text = hashtag_re.sub(\"[hashtag]\",text)\n",
    "    p_text = mention_re.sub(\"[mention]\",p_text)\n",
    "    p_text = url_re.sub(\"[url]\",p_text)\n",
    "    p_text = ftfy.fix_text(p_text)\n",
    "    return p_text.lower()\n",
    "\n",
    "tokenise_re = re.compile(r\"(\\[[^\\]]+\\]|[-'\\w]+|[^\\s\\w\\[']+)\") #([]|words|other non-space)\n",
    "def tokenise(text):\n",
    "    return tokenise_re.findall(text)\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, meta={}):\n",
    "        self.meta = meta\n",
    "        self.tokens_fql = Counter() #empty Counter, ready to be added to with Counter.update.\n",
    "        self.pos_fql = Counter()\n",
    "        self.pos_list = [] #empty list for pos tags from running text.\n",
    "        self.num_tokens = 0\n",
    "        self.text = \"\"\n",
    "\n",
    "        \n",
    "    def extract_features_from_text(self, text):\n",
    "        self.text += text\n",
    "        p_text = preprocess(text)\n",
    "        tokens = tokenise(p_text)\n",
    "        self.num_tokens += len(tokens)\n",
    "        self.tokens_fql.update(tokens) #updating Counter counts items in list, adding to existing Counter items.\n",
    "        pos_tagged = nltk.pos_tag(tokens)\n",
    "        pos = [tag[1] for tag in pos_tagged]\n",
    "        self.pos_fql.update(pos)\n",
    "        self.pos_list.extend(pos)\n",
    "        \n",
    "    def extract_features_from_texts(self, texts): #texts should be iterable text lines, e.g. read in from file.\n",
    "        for text in texts:\n",
    "            extract_features_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, process_method):\n",
    "        self.process_method = process_method\n",
    "    \n",
    "    def fit(self, X, y=None): #no fitting necessary, although could use this to build a vocabulary for all documents, and then limit to set (e.g. top 1000).\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield self.process_method(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_fql(document):\n",
    "    return document.tokens_fql\n",
    "\n",
    "def get_pos_fql(document):\n",
    "    return document.pos_fql\n",
    "\n",
    "def read_list(file):\n",
    "    with open(file) as f:\n",
    "        items = []\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            items.append(line.strip())\n",
    "    return items\n",
    "\n",
    "fws = read_list(\"functionwords.txt\")\n",
    "\n",
    "def get_fws_fql(document):\n",
    "    fws_fql = Counter({t: document.tokens_fql[t] for t in fws}) #dict comprehension, t: fql[t] is token: freq.\n",
    "    return +fws_fql\n",
    "\n",
    "def get_text_stats(document):\n",
    "    ttr = len(document.tokens_fql) / document.num_tokens\n",
    "    return {'avg_token_length': document.average_token_length(), 'ttr': ttr }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the Tweets are imported and put into a `Document` instance for each Tweet. This could be edited easily to use in CountVectorizer (as per week 17's lab), just return a list containing the tweet text, and a list containing the labels. All Tweets available from SemEval data are combined here, allowing for our own train/test split or cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_tweets(file, label):\n",
    "    metadata = {'label': label}\n",
    "    with open(file) as f:\n",
    "        tweets = f.readlines()\n",
    "        for tweet in tweets:\n",
    "            doc = Document(meta=metadata)\n",
    "            doc.extract_features_from_text(tweet)\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "corpus.extend(import_tweets(\"sentiment/all/negative.txt\", \"negative\"))\n",
    "corpus.extend(import_tweets(\"sentiment/all/positive.txt\", \"positive\"))\n",
    "corpus.extend(import_tweets(\"sentiment/all/neutral.txt\", \"neutral\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [d.meta['label'] for d in corpus]\n",
    "X = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor, VotingClassifier, RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state = 0, stratify=y)\n",
    "print(len(X_train), len(X_test))\n",
    "print(len(y_train), len(y_test))\n",
    "\n",
    "model = Pipeline([\n",
    "    ('processor', DocumentProcessor(process_method = get_tokens_fql)),\n",
    "    ('vectorizer', DictVectorizer()),\n",
    "    ('clf', MLPClassifier()),\n",
    "])\n",
    "\n",
    "\n",
    "print('fitting')\n",
    "model.fit(X_train, y_train)\n",
    "print('Scoring')\n",
    "print(model.score(X_test,y_test))\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "confusion_matrix_heatmap(confusion_matrix(y_test,predictions), ['Negative','Positive','Neutral'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Using code and features from previous labs, build and evaluate a sentiment classifier, which classifies individual Tweets into `positive`, `negative` or `neutral`.\n",
    "\n",
    "Marks will be given as follows.\n",
    "- 3 marks will be given for a well implemented and evaluated with at least 2 advanced feature sets (see below).\n",
    "- 2 marks will be given for a completed, and evaluated, basic classifier utilising features from previous labs.\n",
    "- 1 mark will be given for a full attempt.\n",
    "This exercise must be demonstrated in Week 20's lab.\n",
    "\n",
    "Advanced feature sets:\n",
    "- Specific emojis\n",
    "- Just adjectives\n",
    "- Words from a sentiment lexicon (e.g. see \"Opinion Lexicon\" from https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon).\n",
    "- (very advanced) Word embeddings (ask Andrew in Week 19 lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'i': 1,\n",
       "         'wish': 1,\n",
       "         \"they'd\": 1,\n",
       "         'make': 1,\n",
       "         'memorial': 1,\n",
       "         'day': 2,\n",
       "         'and': 1,\n",
       "         'labor': 1,\n",
       "         'on': 1,\n",
       "         'fridays': 1,\n",
       "         '....': 1,\n",
       "         'no': 1,\n",
       "         'real': 1,\n",
       "         'difference': 1,\n",
       "         ',': 1,\n",
       "         'but': 1,\n",
       "         'having': 1,\n",
       "         'a': 2,\n",
       "         'friday': 1,\n",
       "         'off': 1,\n",
       "         'seems': 1,\n",
       "         'so': 1,\n",
       "         'much': 1,\n",
       "         'better': 1,\n",
       "         'than': 1,\n",
       "         'monday': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting\n",
      "Scoring\n",
      "0.5927672955974843\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
