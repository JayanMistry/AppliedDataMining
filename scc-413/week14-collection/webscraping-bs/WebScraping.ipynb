{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "## Extracting text\n",
    "\n",
    "\n",
    "If you use spidering or just download a list of webpage URLs (e.g. with [curl](https://curl.haxx.se/docs/manpage.html) or [requests](http://docs.python-requests.org/en/latest/user/quickstart/)), you will be left with a collection of HTML files. These raw HTML files contain a lot of redundant information (e.g. adverts, menus, headings, etc.), this is known as \"boilerplate\". What we want is the main text of the webpage only (i.e. the news article, the blog post, etc.), in plain text without the HTML tags. If you are not familiar with HTML, [there is a guide here](https://www.w3schools.com/html/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justext\n",
    "\n",
    "Webpages come in all shapes and sizes, and it varies massively how easy it is to scrape the text of interest. If we're lucky, it's relatively easy to pick out the text, and there a fully automated tools available for that. One of these tools is [justext](https://github.com/miso-belica/jusText).\n",
    "\n",
    "1. Have a read through the [description of the justext algorithm](http://corpus.tools/wiki/Justext/Algorithm).\n",
    "2. There is a good [online demo of the tool](https://nlp.fi.muni.cz/projects/justext/). Try out a webpage, e.g. a BBC news article, this article demonstrates it well: <http://www.bbc.co.uk/bbcthree/article/cc72247b-e658-4af8-a838-dfe4e68e2776>.\n",
    "3. Manually compare the filtered text and the text on the web page. How accurate is the text extraction? How much is missed, how much text is incorrectly included. What are the potential impacts of this in a later analysis of the text?\n",
    "4. You can use justext on the command line too. Use it to process a previously gathered webpage, or a sample webpage is available alongside this notebook ([5.html](5.html)), e.g.:\n",
    "```\n",
    "$ python3 -m justext -s English -o 5.txt 5.html\n",
    "```\n",
    "\n",
    "5. Examine the text produced. How much of the text is correctly outputted?\n",
    "6. Note the `<h>` and `<p>` tags, indicating headers and paragraphs. These may be useful, if for instance you are only interested in headers or the actual body of the text, or want to separate them in later analysis. You'll learn next week how you could filter these, e.g. using regular expressions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often an automated approach will not provide accurate enough results. Fortunately, there are other methods available, other than just manually copying and pasting the text. Many tools have been built that assist in parsing web pages and extracting the text of interest, although scripts need writing using these tools for different sets of websites. Some mimic a user's interaction with a website to get to the relevant data (e.g. [Selenium](http://seleniumhq.github.io/selenium/docs/api/py/)). [Scrapy](https://doc.scrapy.org/en/latest/index.html), as used in the other lab execsise, is another good option for grabbing and processing webpages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup\n",
    "\n",
    "Here we will be using the Python requests package, which makes downloading webpages easy: <http://docs.python-requests.org/en/latest/user/quickstart/>\n",
    "\n",
    "As above, this will provide raw HTML files. The key part is extracting the relevant parts of the webpage. For this we will use Beautiful Soup: <https://www.crummy.com/software/BeautifulSoup/bs4/doc/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic process is to look at the HTML of the target webpage and look for ways of drilling down to the elements of interest, with the overall aim of extracting just the specific text of interest. This could be metadata, or actual text for further analysis. There are numerous methods provided by Beautiful Soup, please consult [the documentation]{https://www.crummy.com/software/BeautifulSoup/bs4/doc/} for options available.\n",
    "\n",
    "All websites are different (although some have consistent structures), and often a custom scraper needs to be developed. You can use a standard web browser to look at the information of interest, right-click on the first part of the data of interest and select \"Inspect Element\" (Firefox & Safari) or \"Inspect\" (Chrome), you will then see the HTML code for that element, and the surrounding elements.\n",
    "\n",
    "You can then use Beautiful Soup's functions for drilling down and traversing the relevant parts of the web page. You can also extract links and use the requests package to download further webpages for processing.\n",
    "\n",
    "To demonstrate, we will be parsing Wikipedia pages. As an example we will look to extract plot summaries for Star Trek episodes: <https://en.wikipedia.org/wiki/List_of_Star_Trek:_The_Original_Series_episodes>. We can download this webpage with requests easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://en.wikipedia.org/wiki/List_of_Star_Trek:_The_Original_Series_episodes\"\n",
    "#load webpage\n",
    "req = requests.get(base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use Beautiful Soup to put the HTML webpage into a parseable document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a web browser to view the [Wikipedia page](https://en.wikipedia.org/wiki/List_of_Star_Trek:_The_Original_Series_episodes). We are targetting the list of episodes in the tables starting under \"Season 1 (1966â€“67)\". Right click on the table cell with the first episode title (\"The Man Trap\") and \"Inspect Element\" (or just \"Inspect\" in Chrome). Note that this cell (`td`) is of class \"summary\", as is every title cell, and other cells are not. This is our way in. We are going to collect a list of titles from the table, along with the URL of the Wikipedia page about that episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'The Man Trap', 'url': '/wiki/The_Man_Trap'},\n",
       " {'title': 'Charlie X', 'url': '/wiki/Charlie_X'},\n",
       " {'title': 'Where No Man Has Gone Before',\n",
       "  'url': '/wiki/Where_No_Man_Has_Gone_Before'},\n",
       " {'title': 'The Naked Time', 'url': '/wiki/The_Naked_Time'},\n",
       " {'title': 'The Enemy Within',\n",
       "  'url': '/wiki/The_Enemy_Within_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': \"Mudd's Women\", 'url': '/wiki/Mudd%27s_Women'},\n",
       " {'title': 'What Are Little Girls Made Of?',\n",
       "  'url': '/wiki/What_Are_Little_Girls_Made_Of%3F'},\n",
       " {'title': 'Miri', 'url': '/wiki/Miri_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Dagger of the Mind', 'url': '/wiki/Dagger_of_the_Mind'},\n",
       " {'title': 'The Corbomite Maneuver', 'url': '/wiki/The_Corbomite_Maneuver'},\n",
       " {'title': 'The Menagerie',\n",
       "  'url': '/wiki/The_Menagerie_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'The Conscience of the King',\n",
       "  'url': '/wiki/The_Conscience_of_the_King'},\n",
       " {'title': 'Balance of Terror', 'url': '/wiki/Balance_of_Terror'},\n",
       " {'title': 'Shore Leave',\n",
       "  'url': '/wiki/Shore_Leave_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'The Galileo Seven', 'url': '/wiki/The_Galileo_Seven'},\n",
       " {'title': 'The Squire of Gothos', 'url': '/wiki/The_Squire_of_Gothos'},\n",
       " {'title': 'Arena', 'url': '/wiki/Arena_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Tomorrow Is Yesterday', 'url': '/wiki/Tomorrow_Is_Yesterday'},\n",
       " {'title': 'Court Martial',\n",
       "  'url': '/wiki/Court_Martial_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'The Return of the Archons',\n",
       "  'url': '/wiki/The_Return_of_the_Archons'},\n",
       " {'title': 'Space Seed', 'url': '/wiki/Space_Seed'},\n",
       " {'title': 'A Taste of Armageddon', 'url': '/wiki/A_Taste_of_Armageddon'},\n",
       " {'title': 'This Side of Paradise',\n",
       "  'url': '/wiki/This_Side_of_Paradise_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'The Devil in the Dark', 'url': '/wiki/The_Devil_in_the_Dark'},\n",
       " {'title': 'Errand of Mercy', 'url': '/wiki/Errand_of_Mercy'},\n",
       " {'title': 'The Alternative Factor', 'url': '/wiki/The_Alternative_Factor'},\n",
       " {'title': 'The City on the Edge of Forever',\n",
       "  'url': '/wiki/The_City_on_the_Edge_of_Forever'},\n",
       " {'title': 'Operation: Annihilate!', 'url': '/wiki/Operation:_Annihilate!'},\n",
       " {'title': 'Amok Time', 'url': '/wiki/Amok_Time'},\n",
       " {'title': 'Who Mourns for Adonais?',\n",
       "  'url': '/wiki/Who_Mourns_for_Adonais%3F'},\n",
       " {'title': 'The Changeling',\n",
       "  'url': '/wiki/The_Changeling_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Mirror, Mirror',\n",
       "  'url': '/wiki/Mirror,_Mirror_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'The Apple',\n",
       "  'url': '/wiki/The_Apple_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'The Doomsday Machine',\n",
       "  'url': '/wiki/The_Doomsday_Machine_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Catspaw', 'url': '/wiki/Catspaw_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'I, Mudd', 'url': '/wiki/I,_Mudd'},\n",
       " {'title': 'Metamorphosis',\n",
       "  'url': '/wiki/Metamorphosis_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Journey to Babel', 'url': '/wiki/Journey_to_Babel'},\n",
       " {'title': \"Friday's Child\",\n",
       "  'url': '/wiki/Friday%27s_Child_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'The Deadly Years', 'url': '/wiki/The_Deadly_Years'},\n",
       " {'title': 'Obsession',\n",
       "  'url': '/wiki/Obsession_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Wolf in the Fold', 'url': '/wiki/Wolf_in_the_Fold'},\n",
       " {'title': 'The Trouble with Tribbles',\n",
       "  'url': '/wiki/The_Trouble_with_Tribbles'},\n",
       " {'title': 'The Gamesters of Triskelion',\n",
       "  'url': '/wiki/The_Gamesters_of_Triskelion'},\n",
       " {'title': 'A Piece of the Action',\n",
       "  'url': '/wiki/A_Piece_of_the_Action_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'The Immunity Syndrome',\n",
       "  'url': '/wiki/The_Immunity_Syndrome_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'A Private Little War', 'url': '/wiki/A_Private_Little_War'},\n",
       " {'title': 'Return to Tomorrow', 'url': '/wiki/Return_to_Tomorrow'},\n",
       " {'title': 'Patterns of Force',\n",
       "  'url': '/wiki/Patterns_of_Force_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'By Any Other Name', 'url': '/wiki/By_Any_Other_Name'},\n",
       " {'title': 'The Omega Glory', 'url': '/wiki/The_Omega_Glory'},\n",
       " {'title': 'The Ultimate Computer', 'url': '/wiki/The_Ultimate_Computer'},\n",
       " {'title': 'Bread and Circuses',\n",
       "  'url': '/wiki/Bread_and_Circuses_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Assignment: Earth', 'url': '/wiki/Assignment:_Earth'},\n",
       " {'title': \"Spock's Brain\", 'url': '/wiki/Spock%27s_Brain'},\n",
       " {'title': 'The Enterprise Incident', 'url': '/wiki/The_Enterprise_Incident'},\n",
       " {'title': 'The Paradise Syndrome', 'url': '/wiki/The_Paradise_Syndrome'},\n",
       " {'title': 'And the Children Shall Lead',\n",
       "  'url': '/wiki/And_the_Children_Shall_Lead'},\n",
       " {'title': 'Is There in Truth No Beauty?',\n",
       "  'url': '/wiki/Is_There_in_Truth_No_Beauty%3F'},\n",
       " {'title': 'Spectre of the Gun', 'url': '/wiki/Spectre_of_the_Gun'},\n",
       " {'title': 'Day of the Dove', 'url': '/wiki/Day_of_the_Dove'},\n",
       " {'title': 'For the World Is Hollow and I Have Touched the Sky',\n",
       "  'url': '/wiki/For_the_World_Is_Hollow_and_I_Have_Touched_the_Sky'},\n",
       " {'title': 'The Tholian Web', 'url': '/wiki/The_Tholian_Web'},\n",
       " {'title': \"Plato's Stepchildren\", 'url': '/wiki/Plato%27s_Stepchildren'},\n",
       " {'title': 'Wink of an Eye', 'url': '/wiki/Wink_of_an_Eye'},\n",
       " {'title': 'The Empath', 'url': '/wiki/The_Empath'},\n",
       " {'title': 'Elaan of Troyius', 'url': '/wiki/Elaan_of_Troyius'},\n",
       " {'title': 'Whom Gods Destroy',\n",
       "  'url': '/wiki/Whom_Gods_Destroy_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Let That Be Your Last Battlefield',\n",
       "  'url': '/wiki/Let_That_Be_Your_Last_Battlefield'},\n",
       " {'title': 'The Mark of Gideon', 'url': '/wiki/The_Mark_of_Gideon'},\n",
       " {'title': 'That Which Survives', 'url': '/wiki/That_Which_Survives'},\n",
       " {'title': 'The Lights of Zetar', 'url': '/wiki/The_Lights_of_Zetar'},\n",
       " {'title': 'Requiem for Methuselah', 'url': '/wiki/Requiem_for_Methuselah'},\n",
       " {'title': 'The Way to Eden', 'url': '/wiki/The_Way_to_Eden'},\n",
       " {'title': 'The Cloud Minders', 'url': '/wiki/The_Cloud_Minders'},\n",
       " {'title': 'The Savage Curtain', 'url': '/wiki/The_Savage_Curtain'},\n",
       " {'title': 'All Our Yesterdays',\n",
       "  'url': '/wiki/All_Our_Yesterdays_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Turnabout Intruder', 'url': '/wiki/Turnabout_Intruder'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes = [] #to store the list of episodes.\n",
    "#find and loop through all tds (table cells) with class name ``summary'' (which we know is an episode title)\n",
    "for episode_cell in soup.find_all('td', {'class': 'summary'}):\n",
    "    title = episode_cell.a.text.strip() #Get the actual text from the cell.\n",
    "    episode_url = episode_cell.a['href'] #extract the url\n",
    "    episodes.append({'title': title, 'url': episode_url}) #store in dictionary format\n",
    "    \n",
    "episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the URLs are relative, they can be made absolute with urljoin, using the base url as a reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'The Man Trap',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Man_Trap'},\n",
       " {'title': 'Charlie X', 'url': 'https://en.wikipedia.org/wiki/Charlie_X'},\n",
       " {'title': 'Where No Man Has Gone Before',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Where_No_Man_Has_Gone_Before'},\n",
       " {'title': 'The Naked Time',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Naked_Time'},\n",
       " {'title': 'The Enemy Within',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Enemy_Within_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': \"Mudd's Women\",\n",
       "  'url': 'https://en.wikipedia.org/wiki/Mudd%27s_Women'},\n",
       " {'title': 'What Are Little Girls Made Of?',\n",
       "  'url': 'https://en.wikipedia.org/wiki/What_Are_Little_Girls_Made_Of%3F'},\n",
       " {'title': 'Miri',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Miri_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Dagger of the Mind',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Dagger_of_the_Mind'},\n",
       " {'title': 'The Corbomite Maneuver',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Corbomite_Maneuver'},\n",
       " {'title': 'The Menagerie',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Menagerie_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'The Conscience of the King',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Conscience_of_the_King'},\n",
       " {'title': 'Balance of Terror',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Balance_of_Terror'},\n",
       " {'title': 'Shore Leave',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Shore_Leave_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'The Galileo Seven',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Galileo_Seven'},\n",
       " {'title': 'The Squire of Gothos',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Squire_of_Gothos'},\n",
       " {'title': 'Arena',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Arena_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Tomorrow Is Yesterday',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Tomorrow_Is_Yesterday'},\n",
       " {'title': 'Court Martial',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Court_Martial_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'The Return of the Archons',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Return_of_the_Archons'},\n",
       " {'title': 'Space Seed', 'url': 'https://en.wikipedia.org/wiki/Space_Seed'},\n",
       " {'title': 'A Taste of Armageddon',\n",
       "  'url': 'https://en.wikipedia.org/wiki/A_Taste_of_Armageddon'},\n",
       " {'title': 'This Side of Paradise',\n",
       "  'url': 'https://en.wikipedia.org/wiki/This_Side_of_Paradise_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'The Devil in the Dark',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Devil_in_the_Dark'},\n",
       " {'title': 'Errand of Mercy',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Errand_of_Mercy'},\n",
       " {'title': 'The Alternative Factor',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Alternative_Factor'},\n",
       " {'title': 'The City on the Edge of Forever',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_City_on_the_Edge_of_Forever'},\n",
       " {'title': 'Operation: Annihilate!',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Operation:_Annihilate!'},\n",
       " {'title': 'Amok Time', 'url': 'https://en.wikipedia.org/wiki/Amok_Time'},\n",
       " {'title': 'Who Mourns for Adonais?',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Who_Mourns_for_Adonais%3F'},\n",
       " {'title': 'The Changeling',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Changeling_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Mirror, Mirror',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Mirror,_Mirror_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'The Apple',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Apple_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'The Doomsday Machine',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Doomsday_Machine_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Catspaw',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Catspaw_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'I, Mudd', 'url': 'https://en.wikipedia.org/wiki/I,_Mudd'},\n",
       " {'title': 'Metamorphosis',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Metamorphosis_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Journey to Babel',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Journey_to_Babel'},\n",
       " {'title': \"Friday's Child\",\n",
       "  'url': 'https://en.wikipedia.org/wiki/Friday%27s_Child_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'The Deadly Years',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Deadly_Years'},\n",
       " {'title': 'Obsession',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Obsession_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Wolf in the Fold',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Wolf_in_the_Fold'},\n",
       " {'title': 'The Trouble with Tribbles',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Trouble_with_Tribbles'},\n",
       " {'title': 'The Gamesters of Triskelion',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Gamesters_of_Triskelion'},\n",
       " {'title': 'A Piece of the Action',\n",
       "  'url': 'https://en.wikipedia.org/wiki/A_Piece_of_the_Action_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'The Immunity Syndrome',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Immunity_Syndrome_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'A Private Little War',\n",
       "  'url': 'https://en.wikipedia.org/wiki/A_Private_Little_War'},\n",
       " {'title': 'Return to Tomorrow',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Return_to_Tomorrow'},\n",
       " {'title': 'Patterns of Force',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Patterns_of_Force_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'By Any Other Name',\n",
       "  'url': 'https://en.wikipedia.org/wiki/By_Any_Other_Name'},\n",
       " {'title': 'The Omega Glory',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Omega_Glory'},\n",
       " {'title': 'The Ultimate Computer',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Ultimate_Computer'},\n",
       " {'title': 'Bread and Circuses',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Bread_and_Circuses_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Assignment: Earth',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Assignment:_Earth'},\n",
       " {'title': \"Spock's Brain\",\n",
       "  'url': 'https://en.wikipedia.org/wiki/Spock%27s_Brain'},\n",
       " {'title': 'The Enterprise Incident',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Enterprise_Incident'},\n",
       " {'title': 'The Paradise Syndrome',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Paradise_Syndrome'},\n",
       " {'title': 'And the Children Shall Lead',\n",
       "  'url': 'https://en.wikipedia.org/wiki/And_the_Children_Shall_Lead'},\n",
       " {'title': 'Is There in Truth No Beauty?',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Is_There_in_Truth_No_Beauty%3F'},\n",
       " {'title': 'Spectre of the Gun',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Spectre_of_the_Gun'},\n",
       " {'title': 'Day of the Dove',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Day_of_the_Dove'},\n",
       " {'title': 'For the World Is Hollow and I Have Touched the Sky',\n",
       "  'url': 'https://en.wikipedia.org/wiki/For_the_World_Is_Hollow_and_I_Have_Touched_the_Sky'},\n",
       " {'title': 'The Tholian Web',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Tholian_Web'},\n",
       " {'title': \"Plato's Stepchildren\",\n",
       "  'url': 'https://en.wikipedia.org/wiki/Plato%27s_Stepchildren'},\n",
       " {'title': 'Wink of an Eye',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Wink_of_an_Eye'},\n",
       " {'title': 'The Empath', 'url': 'https://en.wikipedia.org/wiki/The_Empath'},\n",
       " {'title': 'Elaan of Troyius',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Elaan_of_Troyius'},\n",
       " {'title': 'Whom Gods Destroy',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Whom_Gods_Destroy_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Let That Be Your Last Battlefield',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Let_That_Be_Your_Last_Battlefield'},\n",
       " {'title': 'The Mark of Gideon',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Mark_of_Gideon'},\n",
       " {'title': 'That Which Survives',\n",
       "  'url': 'https://en.wikipedia.org/wiki/That_Which_Survives'},\n",
       " {'title': 'The Lights of Zetar',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Lights_of_Zetar'},\n",
       " {'title': 'Requiem for Methuselah',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Requiem_for_Methuselah'},\n",
       " {'title': 'The Way to Eden',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Way_to_Eden'},\n",
       " {'title': 'The Cloud Minders',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Cloud_Minders'},\n",
       " {'title': 'The Savage Curtain',\n",
       "  'url': 'https://en.wikipedia.org/wiki/The_Savage_Curtain'},\n",
       " {'title': 'All Our Yesterdays',\n",
       "  'url': 'https://en.wikipedia.org/wiki/All_Our_Yesterdays_(Star_Trek:_The_Original_Series)'},\n",
       " {'title': 'Turnabout Intruder',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Turnabout_Intruder'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urljoin #https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urljoin\n",
    "for episode in episodes:\n",
    "    episode['url'] = urljoin(base_url,episode['url'])\n",
    "    \n",
    "episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of episodes and their individual wikipedia pages for downloading. We can try justext on the first episode to see if automatic extraction will do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'justext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ee410e13498c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjustext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mepisode_req\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#use requests to download episode webpage.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'justext'"
     ]
    }
   ],
   "source": [
    "import justext\n",
    "episode_req = requests.get(episodes[0]['url']) #use requests to download episode webpage.\n",
    "\n",
    "\n",
    "\n",
    "paragraphs = justext.justext(episode_req.text, justext.get_stoplist(\"English\"))\n",
    "text = \"\"\n",
    "\n",
    "for p in paragraphs:\n",
    "    if not p.is_boilerplate:\n",
    "        text += p.text.strip() + \"\\n\"\n",
    "        \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the [Wikipedia page](https://en.wikipedia.org/wiki/The_Man_Trap). It does a pretty good job of getting the text from the whole page, however, we want to only include the plot section. To do this, we need to be more specific about what to extract, so we can use Beautiful Soup.\n",
    "\n",
    "Looking at the [Wikipedia page](https://en.wikipedia.org/wiki/The_Man_Trap), you can see the sections are headed with an `h2` element, so to find the \"Plot\" section we just need to go through the `h2` tags, find the one with \"Plot\" and hoover up all of the text between there and the next section. We can do this for the first episode as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import Tag #we need the Tag class from Beautiful Soup to check if a node we are looking at is Tag.\n",
    "\n",
    "episode_soup = BeautifulSoup(episode_req.text, \"html.parser\") #use beautiful soup to decode into a parseable document.\n",
    "\n",
    "episode_plot = \"\"\n",
    "\n",
    "for h2 in episode_soup.find_all(\"h2\"): #Go through all of the h2 elements.\n",
    "            if(h2.text.strip().startswith(\"Plot\")): #This is the h2 With \"Plot\" (and \"Plot Summary\")\n",
    "                node = h2.next_sibling #start looking for tags after the Plot h2, will be strings and Tags.\n",
    "                while True:\n",
    "                    if isinstance(node, Tag): #Check if this element is actually a Tag.\n",
    "                        if node.name == \"p\": #p tag, we want this.\n",
    "                            episode_plot += node.text.strip() + \"\\n\" #append the text from p.\n",
    "                        elif node.name == \"h2\": #at the next h2, so a new section, no longer the plot. Stop processing.\n",
    "                            break\n",
    "                    node = node.next_sibling #get next element at same level.\n",
    "                    \n",
    "print(episode_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see we now just have the plot text. We just need to wrap this up in a loop and add the plot to each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for episode in episodes:\n",
    "    episode_plot = \"\"\n",
    "    episode_req = requests.get(episode['url']) #do a new request for the episode page.\n",
    "    episode_soup = BeautifulSoup(episode_req.text, \"html.parser\") #use beautiful soup to decode into a parseable document.\n",
    "    for h2 in episode_soup.find_all(\"h2\"): #Go through all of the h2 elements.\n",
    "        if(h2.text.strip().startswith(\"Plot\")): #This is the h2 With \"Plot\" (and \"Plot Summary\")\n",
    "            node = h2.next_sibling #start looking for tags after the Plot h2, will be strings and Tags.\n",
    "            while True:\n",
    "                if isinstance(node, Tag): #Check if this element is actually a Tag.\n",
    "                    if node.name == \"p\": #p tag, we want this.\n",
    "                        episode_plot += node.text.strip() + \"\\n\" #append the text from p.\n",
    "                    elif node.name == \"h2\": #at the next h2, so a new section, no longer the plot. Stop processing.\n",
    "                        break\n",
    "                node = node.next_sibling #get next element at same level.\n",
    "\n",
    "        episode['plot'] = episode_plot #add the plot to episode.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is in dictionary format, it's nice to export to JSON. Simple to change to outputting to file (see e.g. Twitter task: tweets_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(episodes,indent=4)) #print out the resulting json (pretty printed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is split up here to explain it more neatly in a notebook, the the full code is available in [startrekscrape.py](startrekscrape.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Exercise\n",
    "\n",
    "To practice using Beautiful Soup, try extracting details of films by Stanley Kubrick from this page: <https://en.wikipedia.org/wiki/Filmography_and_awards_of_Stanley_Kubrick>. The aim is to extract the year and title for the 17 films listed. This is a little tricky as the year spans two rows in some cases. Some starting code is provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1951\n",
      "\n",
      "Day of the Fight\n",
      "\n",
      "Flying Padre\n",
      "\n",
      "\n",
      "\n",
      "1953\n",
      "\n",
      "Fear and Desire\n",
      "\n",
      "The Seafarers\n",
      "\n",
      "\n",
      "\n",
      "1955\n",
      "\n",
      "Killer's Kiss\n",
      "\n",
      "1956\n",
      "\n",
      "The Killing\n",
      "\n",
      "1957\n",
      "\n",
      "Paths of Glory\n",
      "\n",
      "1960\n",
      "\n",
      "Spartacus\n",
      "\n",
      "1962\n",
      "\n",
      "Lolita\n",
      "\n",
      "1964\n",
      "\n",
      "Dr. Strangelove\n",
      "\n",
      "1968\n",
      "\n",
      "2001: A Space Odyssey\n",
      "\n",
      "1971\n",
      "\n",
      "A Clockwork Orange\n",
      "\n",
      "1975\n",
      "\n",
      "Barry Lyndon\n",
      "\n",
      "1980\n",
      "\n",
      "The Shining\n",
      "\n",
      "1987\n",
      "\n",
      "Full Metal Jacket\n",
      "\n",
      "1999\n",
      "\n",
      "Eyes Wide Shut\n",
      "\n",
      "2001\n",
      "\n",
      "A.I. Artificial Intelligence\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://en.wikipedia.org/wiki/Filmography_and_awards_of_Stanley_Kubrick\"\n",
    "#load webpage\n",
    "req = requests.get(base_url)\n",
    "#Use beautiful soup to decode webpage text into parseable document.\n",
    "soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "table = soup.find('table', {'class': 'wikitable'}) #just find 1 table, as it's the first 1\n",
    "trs = table.findAll('tr') #all table rows\n",
    "trs = trs[1:] #skip header row\n",
    "\n",
    "\n",
    "for row in trs:\n",
    "    td_value = row.find_all(\"td\")\n",
    "    year  = td_value[0].text\n",
    "    title = td_value[1].text\n",
    "    year = year.replace(\"Yes\",\"\") \n",
    "    title = title.replace(\"Yes\",\"\")   \n",
    "    \n",
    "    print(year)\n",
    "    print(title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: You can use 'rowspan' to see if a cell covers multiple rows. You can also use [findAll with a limit](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#the-limit-argument) to return only a limited number of nodes from another node, e.g. `td`s in a `tr`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced\n",
    "\n",
    "You can also use what you've learnt to parse details of something else from Wikipedia, e.g. other TV series, or albums from an artist's discography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forum Exercise\n",
    "\n",
    "The other lab workbook uses Scrapy to download pages from a forum, you can perform a similar task for forums with Requests and Beautiful Soup. You can choose any forum and use what you've learnt to parse thread posts into plain text. Start with an individual thread, and 1 page of posts within that thread. A good example to try is from Mumsnet on noisy baby toys (because... why not?): <https://www.mumsnet.com/Talk/toys_and_games_chat/3414974-noisy-baby-toys-which-are-the-worst>.\n",
    "\n",
    "1. Try using justext to parse the the first page of posts, examine the results. Is this good enough?\n",
    "2. Use Beautiful Soup to collect just the posts from the first page and output as plain text.\n",
    "\n",
    "Hint: one issue you may come across is `<br>` tags instead of newlines. If ignored, this will run lines of text in the same post directly next to each other (without even a space), e.g. something horrible like this:\n",
    "\"Allllllll of the toot tootSpin and bounce zebraAnd best (worst!) of all... peppa pig alphaphonic board- it has no off switch and the slightest touch sets it off for ages...\"\n",
    "\n",
    "This makes tokenisation (see next week) difficult and is tricky to rectify. This is a good lesson in sanity checking the exported text, better to have the text as close to as appears on the webpage now. To deal with this issue, the `<br>` tags can be replaced with a new line character (or some other marker), e.g.:\n",
    "\n",
    "```\n",
    "for br in post.find_all(\"br\"):\n",
    "    br.replace_with(\"\\n\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \n",
      " My best friend has a 6 month old and after 14 years of her gleefully finding the biggest most annoying toys she could for my children I am desperate to get my own back.  What are the current most ear shattering noisy toys ( preferably plastic with flashing lights) that you can buy now.  I remember vtech baby walkers were pretty appalling when mine were small. \n",
      " ,  \n",
      " Following with interest, I'm in a similar situation   \n",
      " ,  \n",
      " Might just be us  @Sjjr23   I'm thinking the pink vtech walker looks horrific so might go for that \n",
      " ,  \n",
      " I'm bumping this as I refuse to believe we are the only 2 vengeful people on here   \n",
      " ,  \n",
      "  My 18 month is currently playing on her Vtech bounce and spin frog. It's very noisy I have the subtitles on TV . The volume has 2 settings though so she can always just switch it off  \n",
      " ,  \n",
      " Seems like vetch is the best/worst gift to give \n",
      " ,  \n",
      " Lamaze Sunny Rabbit. It doesn't have an off button ðŸ˜¡ \n",
      " ,  \n",
      "  The fisher price cookie jar is awful! One of those that the song gets stuck in your head. \n",
      " ,  \n",
      " To be honestly they're all horrendous after a while. The worst we have is a flashing drum kit and keyboard combination. It's so loud ðŸ™‰ \n",
      " ,  \n",
      " Ahhhh. You need to speak to my father about this. He relishes in finding the gifts parents dread and gives them to my children every year. The highlight was when he bought them an indoor rollercoaster and an electric car the year I lived in a tiny two bed flat.  V tech is totally the way to go. I still have the cookie jar songs in my head, and all the vtech toot toot cars are appalling too. Sometimes they go off in the middle of the night and I have a heart attack, which is an added joy. \n",
      " ,  \n",
      " Also, FWIW, wait til the kid gets older. Buy them a violin. It's the ultimate way to win the game. \n",
      " ,  \n",
      "  The Fisher Price laugh and learn puppy has the most irritating squeaky voice and haunts my nightmares \n",
      " ,  \n",
      " Toot toot!! The stuff drives me insane... Yes the spinny frog thing.. the pink walker was not too bad as it can be turned off. \n",
      " ,  \n",
      "  Anything vetch did my head in! Especially the toot toot vehicles which would suddenly start singing from the bottom of the toy box at random times for no reason. \n",
      " ,  \n",
      "  I've bought mine the cookie jar for Christmas. I can still remember the song from when my 7 year old had it at the same age. I don't remember it being that bad oh dear   \n",
      " ,  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  This! Really bloody annoying. \n",
      " ,  \n",
      "  Itâ€™s a V Tech, by the way. \n",
      " ,  \n",
      "  www.amazon.co.uk/Early-Learning-Centre-Singing-Maracas/dp/B07F41Z5FK?tag=mumsnetforum-21   Or these, Holy Mother of God, the noise. \n",
      " ,  \n",
      " Did you go for the Pink VTec walker? It's horrendous   \"Welcome to our learning farm we have lots to show you\" is the most horrendous song.  My in laws (FIL, BIL &amp; SIL) have all bought my 2 year old the most horrendously noisy toys for her birthday a few weeks ago  Leapfrog count along til with the worst song \"In the dairy aisle I'm shopping along in the dairy aisle, look what I've found\"  And not one but two Phonetics toys, one shaped like an apple and another one which is like an orchastra and the letters sing their sound   \n",
      " ,  \n",
      " My daughter presses the 'hang up' button over and over and over on her V Tech walker... (You know where the phone  used  to be as after 5 minutes it's lost anyway.)  \"Thanks for calling\" sometimes not even being able to finish her sentence... on a loop for 15 minutes, it's delightful.  Deffo get the pink walker ï¿¼ \n",
      " ,  \n",
      "  Oh also remembered this. It plays the same couple of tunes and lights up. Very noisy. I got it for about Â£20/Â£30 not $99.00 dollars I just clicked the first link. It changes for when the baby is standing so lasts a long time. \n",
      " ,  \n",
      "  www.lamkins.com.sg/fisher-price-grow-with-gym-p-5128.html   I had Vtech walkers for both of mine and never put batteries in either  too lazy . I feel like I've missed out   \n",
      " ,  \n",
      " Anything v tech, the walker, the toot toot cars and buildings. We also have this steering wheel to attach to a pram. My DS loves pressing the same music button over and over and over and over and over and over again. Early Learning Centre Figurines (Lights and Sounds Buggy Driver)  https://www.amazon.co.uk/dp/B0063DEIQK/ref=cm  sw w_ r  cp p_ api_4wi6BbQP06S31  \n",
      " ,  \n",
      " Vtech little singing Alfie Bear - that thing used to drive me completely insane! \n",
      " ,  \n",
      " Hello puppy c-calling do you want to play with me  Letâ€™s have fun together while we learn our ABC  8 fucking years ago. 8 years. \n",
      " ]\n",
      "[\n",
      "My best friend has a 6 month old and after 14 years of her gleefully finding the biggest most annoying toys she could for my children I am desperate to get my own back. What are the current most ear shattering noisy toys ( preferably plastic with flashing lights) that you can buy now. I remember vtech baby walkers were pretty appalling when mine were small.\n",
      ", \n",
      "Following with interest, I'm in a similar situation \n",
      ", \n",
      "Might just be us @Sjjr23 I'm thinking the pink vtech walker looks horrific so might go for that\n",
      ", \n",
      "I'm bumping this as I refuse to believe we are the only 2 vengeful people on here \n",
      ", \n",
      " My 18 month is currently playing on her Vtech bounce and spin frog. It's very noisy I have the subtitles on TV . The volume has 2 settings though so she can always just switch it off\n",
      ", \n",
      "Seems like vetch is the best/worst gift to give\n",
      ", \n",
      "Lamaze Sunny Rabbit. It doesn't have an off button ðŸ˜¡\n",
      ", \n",
      " The fisher price cookie jar is awful! One of those that the song gets stuck in your head.\n",
      ", \n",
      "To be honestly they're all horrendous after a while. The worst we have is a flashing drum kit and keyboard combination. It's so loud ðŸ™‰\n",
      ", \n",
      "Ahhhh. You need to speak to my father about this. He relishes in finding the gifts parents dread and gives them to my children every year. The highlight was when he bought them an indoor rollercoaster and an electric car the year I lived in a tiny two bed flat. V tech is totally the way to go. I still have the cookie jar songs in my head, and all the vtech toot toot cars are appalling too. Sometimes they go off in the middle of the night and I have a heart attack, which is an added joy.\n",
      ", \n",
      "Also, FWIW, wait til the kid gets older. Buy them a violin. It's the ultimate way to win the game.\n",
      ", \n",
      " The Fisher Price laugh and learn puppy has the most irritating squeaky voice and haunts my nightmares\n",
      ", \n",
      "Toot toot!! The stuff drives me insane... Yes the spinny frog thing.. the pink walker was not too bad as it can be turned off.\n",
      ", \n",
      " Anything vetch did my head in! Especially the toot toot vehicles which would suddenly start singing from the bottom of the toy box at random times for no reason.\n",
      ", \n",
      " I've bought mine the cookie jar for Christmas. I can still remember the song from when my 7 year old had it at the same age. I don't remember it being that bad oh dear \n",
      ", \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " This! Really bloody annoying.\n",
      ", \n",
      " Itâ€™s a V Tech, by the way.\n",
      ", \n",
      "www.amazon.co.uk/Early-Learning-Centre-Singing-Maracas/dp/B07F41Z5FK?tag=mumsnetforum-21Or these, Holy Mother of God, the noise.\n",
      ", \n",
      "Did you go for the Pink VTec walker? It's horrendous \"Welcome to our learning farm we have lots to show you\" is the most horrendous song.My in laws (FIL, BIL & SIL) have all bought my 2 year old the most horrendously noisy toys for her birthday a few weeks agoLeapfrog count along til with the worst song \"In the dairy aisle I'm shopping along in the dairy aisle, look what I've found\"And not one but two Phonetics toys, one shaped like an apple and another one which is like an orchastra and the letters sing their sound \n",
      ", \n",
      "My daughter presses the 'hang up' button over and over and over on her V Tech walker... (You know where the phone used to be as after 5 minutes it's lost anyway.)\"Thanks for calling\" sometimes not even being able to finish her sentence... on a loop for 15 minutes, it's delightful.Deffo get the pink walker ï¿¼\n",
      ", \n",
      " Oh also remembered this. It plays the same couple of tunes and lights up. Very noisy. I got it for about Â£20/Â£30 not $99.00 dollars I just clicked the first link. It changes for when the baby is standing so lasts a long time.\n",
      ", \n",
      "www.lamkins.com.sg/fisher-price-grow-with-gym-p-5128.htmlI had Vtech walkers for both of mine and never put batteries in either too lazy. I feel like I've missed out \n",
      ", \n",
      "Anything v tech, the walker, the toot toot cars and buildings. We also have this steering wheel to attach to a pram. My DS loves pressing the same music button over and over and over and over and over and over again. Early Learning Centre Figurines (Lights and Sounds Buggy Driver) https://www.amazon.co.uk/dp/B0063DEIQK/ref=cmsww_rcpp_api_4wi6BbQP06S31\n",
      ", \n",
      "Vtech little singing Alfie Bear - that thing used to drive me completely insane!\n",
      ", \n",
      "Hello puppy c-calling do you want to play with meLetâ€™s have fun together while we learn our ABC8 fucking years ago. 8 years.\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import requests\n",
    "\n",
    "r  = requests.get(\"https://www.mumsnet.com/Talk/toys_and_games_chat/3414974-noisy-baby-toys-which-are-the-worst\")\n",
    "\n",
    "data = r.text\n",
    "\n",
    "soup = BeautifulSoup(data)\n",
    "\n",
    "posts = soup.find_all(\"div\", {\"class\": \"message\"})\n",
    "\n",
    "#Using regex\n",
    "regexStripped = re.sub(\"<.*?>\", \" \", str(posts))\n",
    "print(regexStripped)\n",
    "\n",
    "#Using soup\n",
    "soup = BeautifulSoup(str(posts))\n",
    "all_text = ''.join(soup.findAll(text=True))\n",
    "print(all_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced\n",
    "\n",
    "You will notice that the topic's posts are spread across multiple pages, this means that \"paging\" needs to be performed to extract the posts from every page. This is a little tricky, but work from collecting 1 page. You will need to consult the [requests documentation](http://docs.python-requests.org/en/latest/user/quickstart/) to discover how to pass parameters to match the links to other pages. Be careful not to have duplicate posts in your extraction (the original post is repeated on each page).\n",
    "\n",
    "The trickiest part is to know when to stop. Going past the number of pages available just brings the user to the final page. We will be covering regular expressions next week, so to help you out, the following code can be used to find the final page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \n",
      " My best friend has a 6 month old and after 14 years of her gleefully finding the biggest most annoying toys she could for my children I am desperate to get my own back.  What are the current most ear shattering noisy toys ( preferably plastic with flashing lights) that you can buy now.  I remember vtech baby walkers were pretty appalling when mine were small. \n",
      " ,  \n",
      " Following with interest, I'm in a similar situation   \n",
      " ,  \n",
      " Might just be us  @Sjjr23   I'm thinking the pink vtech walker looks horrific so might go for that \n",
      " ,  \n",
      " I'm bumping this as I refuse to believe we are the only 2 vengeful people on here   \n",
      " ,  \n",
      "  My 18 month is currently playing on her Vtech bounce and spin frog. It's very noisy I have the subtitles on TV . The volume has 2 settings though so she can always just switch it off  \n",
      " ,  \n",
      " Seems like vetch is the best/worst gift to give \n",
      " ,  \n",
      " Lamaze Sunny Rabbit. It doesn't have an off button ðŸ˜¡ \n",
      " ,  \n",
      "  The fisher price cookie jar is awful! One of those that the song gets stuck in your head. \n",
      " ,  \n",
      " To be honestly they're all horrendous after a while. The worst we have is a flashing drum kit and keyboard combination. It's so loud ðŸ™‰ \n",
      " ,  \n",
      " Ahhhh. You need to speak to my father about this. He relishes in finding the gifts parents dread and gives them to my children every year. The highlight was when he bought them an indoor rollercoaster and an electric car the year I lived in a tiny two bed flat.  V tech is totally the way to go. I still have the cookie jar songs in my head, and all the vtech toot toot cars are appalling too. Sometimes they go off in the middle of the night and I have a heart attack, which is an added joy. \n",
      " ,  \n",
      " Also, FWIW, wait til the kid gets older. Buy them a violin. It's the ultimate way to win the game. \n",
      " ,  \n",
      "  The Fisher Price laugh and learn puppy has the most irritating squeaky voice and haunts my nightmares \n",
      " ,  \n",
      " Toot toot!! The stuff drives me insane... Yes the spinny frog thing.. the pink walker was not too bad as it can be turned off. \n",
      " ,  \n",
      "  Anything vetch did my head in! Especially the toot toot vehicles which would suddenly start singing from the bottom of the toy box at random times for no reason. \n",
      " ,  \n",
      "  I've bought mine the cookie jar for Christmas. I can still remember the song from when my 7 year old had it at the same age. I don't remember it being that bad oh dear   \n",
      " ,  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  This! Really bloody annoying. \n",
      " ,  \n",
      "  Itâ€™s a V Tech, by the way. \n",
      " ,  \n",
      "  www.amazon.co.uk/Early-Learning-Centre-Singing-Maracas/dp/B07F41Z5FK?tag=mumsnetforum-21   Or these, Holy Mother of God, the noise. \n",
      " ,  \n",
      " Did you go for the Pink VTec walker? It's horrendous   \"Welcome to our learning farm we have lots to show you\" is the most horrendous song.  My in laws (FIL, BIL &amp; SIL) have all bought my 2 year old the most horrendously noisy toys for her birthday a few weeks ago  Leapfrog count along til with the worst song \"In the dairy aisle I'm shopping along in the dairy aisle, look what I've found\"  And not one but two Phonetics toys, one shaped like an apple and another one which is like an orchastra and the letters sing their sound   \n",
      " ,  \n",
      " My daughter presses the 'hang up' button over and over and over on her V Tech walker... (You know where the phone  used  to be as after 5 minutes it's lost anyway.)  \"Thanks for calling\" sometimes not even being able to finish her sentence... on a loop for 15 minutes, it's delightful.  Deffo get the pink walker ï¿¼ \n",
      " ,  \n",
      "  Oh also remembered this. It plays the same couple of tunes and lights up. Very noisy. I got it for about Â£20/Â£30 not $99.00 dollars I just clicked the first link. It changes for when the baby is standing so lasts a long time. \n",
      " ,  \n",
      "  www.lamkins.com.sg/fisher-price-grow-with-gym-p-5128.html   I had Vtech walkers for both of mine and never put batteries in either  too lazy . I feel like I've missed out   \n",
      " ,  \n",
      " Anything v tech, the walker, the toot toot cars and buildings. We also have this steering wheel to attach to a pram. My DS loves pressing the same music button over and over and over and over and over and over again. Early Learning Centre Figurines (Lights and Sounds Buggy Driver)  https://www.amazon.co.uk/dp/B0063DEIQK/ref=cm  sw w_ r  cp p_ api_4wi6BbQP06S31  \n",
      " ,  \n",
      " Vtech little singing Alfie Bear - that thing used to drive me completely insane! \n",
      " ,  \n",
      " Hello puppy c-calling do you want to play with me  Letâ€™s have fun together while we learn our ABC  8 fucking years ago. 8 years. \n",
      " ]\n",
      "re.compile('page (\\\\d+) of \\\\1')\n"
     ]
    }
   ],
   "source": [
    "#this isn't complete code, you'll need to incorporate it into your solution.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import requests\n",
    "\n",
    "results = []\n",
    "\n",
    "lastpage = re.compile(r\"page (\\d+) of \\1\") #compiled regular expression, checking if we're at page n of n, i.e. last page.\n",
    "\n",
    "url = \"https://www.mumsnet.com/Talk/toys_and_games_chat/3414974-noisy-baby-toys-which-are-the-worst?pg=1\"\n",
    "\n",
    "for i in range()\n",
    "r  = requests.get(url)\n",
    "\n",
    "data = r.text\n",
    "\n",
    "soup = BeautifulSoup(data)\n",
    "\n",
    "posts = soup.find_all(\"div\", {\"class\": \"message\"})\n",
    "\n",
    "#Using regex\n",
    "regexStripped = re.sub(\"<.*?>\", \" \", str(posts))\n",
    "print(regexStripped)\n",
    "\n",
    "lastpage = re.compile(r\"page (\\d+) of \\1\") #compiled regular expression, checking if we're at page n of n, i.e. last page.\n",
    "\n",
    "#...\n",
    "\n",
    "pages = soup.find('div', {'class': 'pages'}).text.strip() #find pages element which has the \"This is page x of y\" text.\n",
    "if lastpage.search(pages) != None: #if our regex matches, then we're on last page, so make this the last one to parse.\n",
    "    at_last = True\n",
    "    \n",
    "print(lastpage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go further by parsing multiple threads from a (page of a) subforum (or \"Talk\" on Mumsnet), e.g. try extracting all of the threads from the first page from baby names discussions: <https://www.mumsnet.com/Talk/baby_names>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
