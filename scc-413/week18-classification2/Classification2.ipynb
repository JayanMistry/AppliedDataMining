{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification (Part 2)\n",
    "\n",
    "In the the first classification exercise scikit-learn was utilised to classify Twitter users as male or female. Vectorization was done via sklearn's CountVectorizer. At the end of the lab exercise, the preprocessing and tokenisation used by CountVectorizer were customised to give greater control of how the text was processed and features extracted. This actually allows for quite a number of features to be implemented, e.g.:\n",
    "\n",
    "- POS tags, by pos-tagging during tokenisation, and returning the pos-tags instead of words.\n",
    "- Function words, by setting the vocabulary of CountVectorizer to a function word list.\n",
    "- Hashtags, mentions and/or emojis, by only returning these in the token list.\n",
    "- Characters/graphemes, rather than codepoints, by \"tokenising\" with \"\\X\".\n",
    "\n",
    "But we are limited to counting things. Another issue is that the processing will be done multiple times each time the pipeline is ran (e.g. for gridsearch).\n",
    "\n",
    "In this lab you will learn how to process texts in advance, saving the token lists and frequency lists produced, and allowing for other features to be extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we do all the imports needed in one go. You may import other packages as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, splitext, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of methods for showing classifier results (from 1st classification lab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cv_scores_summary(name, scores):\n",
    "    print(\"{}: mean = {:.2f}%, sd = {:.2f}%, min = {:.2f}, max = {:.2f}\".format(name, scores.mean()*100, scores.std()*100, scores.min()*100, scores.max()*100))\n",
    "    \n",
    "def confusion_matrix_heatmap(cm, index):\n",
    "    cmdf = pd.DataFrame(cm, index = index, columns=index)\n",
    "    dims = (5, 5)\n",
    "    fig, ax = plt.subplots(figsize=dims)\n",
    "    sns.heatmap(cmdf, annot=True, cmap=\"coolwarm\", center=0)\n",
    "    ax.set_ylabel('Actual')    \n",
    "    ax.set_xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For POS tagger, incase these haven't been previously downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('maxent_treebank_pos_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define our document object, along preprocessing tokenisation methods. These are taken (and slightly extended) from the Feature Extraction lab. The document will represent an instance in our classifier, e.g. it could be a collection of a user's Tweets, a single tweet, a longer article, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_re = re.compile(r\"#\\w+\")\n",
    "mention_re = re.compile(r\"@\\w+\")\n",
    "url_re = re.compile(r\"(?:https?://)?(?:[-\\w]+\\.)+[a-zA-Z]{2,9}[-\\w/#~:;.?+=&%@~]*\")\n",
    "\n",
    "def preprocess(text):\n",
    "    p_text = hashtag_re.sub(\"[hashtag]\",text)\n",
    "    p_text = mention_re.sub(\"[mention]\",p_text)\n",
    "    p_text = url_re.sub(\"[url]\",p_text)\n",
    "    p_text = ftfy.fix_text(p_text)\n",
    "    return p_text.lower()\n",
    "\n",
    "tokenise_re = re.compile(r\"(\\[[^\\]]+\\]|[-'\\w]+|[^\\s\\w\\[']+)\") #([]|words|other non-space)\n",
    "def tokenise(text):\n",
    "    return tokenise_re.findall(text)\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, meta={}):\n",
    "        self.meta = meta\n",
    "        self.tokens_fql = Counter() #empty Counter, ready to be added to with Counter.update.\n",
    "        self.pos_fql = Counter()\n",
    "        self.pos_list = [] #empty list for pos tags from running text.\n",
    "        self.num_tokens = 0\n",
    "        \n",
    "    def extract_features_from_text(self, text):\n",
    "        p_text = preprocess(text)\n",
    "        tokens = tokenise(p_text)\n",
    "        self.num_tokens += len(tokens)\n",
    "        self.tokens_fql.update(tokens) #updating Counter counts items in list, adding to existing Counter items.\n",
    "        pos_tagged = nltk.pos_tag(tokens)\n",
    "        pos = [tag[1] for tag in pos_tagged]\n",
    "        self.pos_fql.update(pos)\n",
    "        self.pos_list.extend(pos)\n",
    "        \n",
    "    def extract_features_from_texts(self, texts): #texts should be iterable text lines, e.g. read in from file.\n",
    "        for text in texts:\n",
    "            extract_features_from_text(text)\n",
    "            \n",
    "    def average_token_length(self):\n",
    "        sum_lengths = 0\n",
    "        for key, value in self.tokens_fql.items():\n",
    "            sum_lengths += len(key) * value\n",
    "        return sum_lengths / self.num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Twitter GB celebs again, but from JSON files with further metadata available (the data is slightly different from the first classification lab as it was collected later). The method below reads in the json file, extracts the metadata for the user, creates a new Document representing the user, and adds all of the Tweets (extracting features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_celebs_json(folder):\n",
    "    jsonfiles = [join(folder, f) for f in listdir(folder) if isfile(join(folder, f)) and f.endswith(\".json\")]\n",
    "    for jf in jsonfiles:\n",
    "        with open(jf) as f:\n",
    "            data = json.load(f)\n",
    "            handle = data['handle']\n",
    "            gender = data['gender']\n",
    "            age_range = data['age_range']\n",
    "            english = data['english']\n",
    "        print(\"Processing \" + handle)\n",
    "        doc = Document({'handle': handle, 'gender': gender, 'age_range': age_range, 'english': english}) #include metadata\n",
    "        for tweet in data['tweets']:\n",
    "            doc.extract_features_from_text(tweet['text'])\n",
    "        yield doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the corpus by processing the folder of gb celebs. This will take a little while due to the pos-tagging, but it should only need doing once (unless you change the Document class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "corpus.extend(import_celebs_json(\"celebs-gb-json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the gender metadata from the Documents as our label/class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [d.meta['gender'] for d in corpus]\n",
    "X = corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform the usual train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state = 0, stratify=y)\n",
    "print(len(X_train), len(X_test))\n",
    "print(len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our train/test instances in our custom class to hold the features, we need to build a custom `Transformer` which takes in one dataset and returns a new dataset. Here we need to take in a list of `Document` objects and transform it into a set of features. We build a simple class for this, which overrides the transform method. The intention is for a list of `Document` objects to be passed into the transformer, and parameter-defined (callable) method is used to extract the featuress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, process_method):\n",
    "        self.process_method = process_method\n",
    "    \n",
    "    def fit(self, X, y=None): #no fitting necessary, although could use this to build a vocabulary for all documents, and then limit to set (e.g. top 1000).\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents):\n",
    "        for document in documents:\n",
    "            yield self.process_method(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some example methods for returning the token frequency list, pos frequency list, or some text statistics from the `Document`. These can be edited and added to as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_fql(document):\n",
    "    return document.tokens_fql\n",
    "\n",
    "def get_pos_fql(document):\n",
    "    return document.pos_fql\n",
    "\n",
    "def get_text_stats(document):\n",
    "    ttr = len(document.tokens_fql) / document.num_tokens\n",
    "    return {'avg_token_length': document.average_token_length(), 'ttr': ttr }\n",
    "\n",
    "def read_list(file):\n",
    "    with open(file) as f:\n",
    "        items = []\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            items.append(line.strip())\n",
    "    return items\n",
    "\n",
    "fws = read_list(\"functionwords.txt\")\n",
    "\n",
    "def get_fws_fql(document):\n",
    "    fws_fql = Counter({t: document.tokens_fql[t] for t in fws}) #dict comprehension, t: fql[t] is token: freq.\n",
    "    return +fws_fql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build pipeline with the `DocumentProcessor` as the first step, extract POS tag frequencies as features. The output from the new `DocumentProcessor` is then passed to a [`DictVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer), which transforms the features into a vector (scipy.sparse matrix), which can be used with other sklearn steps. To demontrate, we pass to a Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('processor', DocumentProcessor(process_method = get_pos_fql)),\n",
    "    ('vectorizer', DictVectorizer()),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_validate(model, X, y, cv=StratifiedKFold(n_splits=5, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Union\n",
    "\n",
    "All the pipelines we have used so far have been simple linear sequences of steps. With [`FeatureUnion`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion) we can have complex branched sequences, where different features, or different parts of the text are processed separately. The results are then concatenated together into a composite vector. We could, for example, read in a user's bio separately from the rest of their text, we could even utilise metadata (such as usernames to predict gender), or other data such as profile images.\n",
    "\n",
    "Below we utilise different featuresets available from the Document instances and concatenate them together simply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('text_union', FeatureUnion(\n",
    "        transformer_list = [\n",
    "            ('pos_features', Pipeline([\n",
    "                ('pos_processor', DocumentProcessor(process_method = get_pos_fql)),\n",
    "                ('pos_vectorizer', DictVectorizer()),\n",
    "            ])),\n",
    "            ('fws_features', Pipeline([\n",
    "                ('fws_processor', DocumentProcessor(process_method = get_fws_fql)),\n",
    "                ('fws_vectorizer', DictVectorizer()),\n",
    "            ])),\n",
    "            ('stats_features', Pipeline([\n",
    "                ('stats_processor', DocumentProcessor(process_method = get_text_stats)),\n",
    "                ('stats_vectorizer', DictVectorizer()),\n",
    "            ])),\n",
    "        ],\n",
    "    )),\n",
    "    ('clf', LogisticRegression(solver='liblinear', random_state=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_validate(model, X, y, cv=StratifiedKFold(n_splits=5, random_state=0), return_train_score=False, scoring=['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "print_cv_scores_summary(\"Accuracy\", cv_scores['test_accuracy'])\n",
    "print_cv_scores_summary(\"Precision\", cv_scores['test_precision_weighted'])\n",
    "print_cv_scores_summary(\"Recall\", cv_scores['test_recall_weighted'])\n",
    "print_cv_scores_summary(\"F1\", cv_scores['test_f1_weighted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying other labels\n",
    "\n",
    "We have been working on binary classification of gender. Other user metadata can be predicted also, with the age_range and English variety being present (if you use the larger datasets provided).\n",
    "\n",
    "Looking at age, there are 5 age ranges present, plus some marked as \"unknown\". We need to remove the \"unknowns\" as predicting as 'unknown' does not make sense. You could predict these unknowns with your trained classifier at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([d.meta['age_range'] for d in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_X = [d for d in X if d.meta['age_range'] != 'unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(age_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 instances have been removed. We need to extract the new labels also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_y = [d.meta['age_range'] for d in age_X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a train-test split as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_X_train, age_X_test, age_y_train, age_y_test = train_test_split(age_X, age_y, test_size=0.2, random_state = 0, stratify=age_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a pipeline with feature union similar to earlier, except we are now also using grid search, including if to include funtion words or all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list = [\n",
    "            ('pos', Pipeline([\n",
    "                ('processor', DocumentProcessor(process_method = get_pos_fql)),\n",
    "                ('vectorizer', DictVectorizer()),\n",
    "            ])),\n",
    "            ('word', Pipeline([\n",
    "                ('processor', DocumentProcessor(process_method = None)), # to be set by grid search.\n",
    "                ('vectorizer', DictVectorizer()),\n",
    "            ])),\n",
    "            ('stats', Pipeline([\n",
    "                ('processor', DocumentProcessor(process_method = get_text_stats)),\n",
    "                ('vectorizer', DictVectorizer()),\n",
    "            ])),\n",
    "        ],\n",
    "    )),\n",
    "    ('selector', SelectKBest(score_func = chi2)),\n",
    "    ('clf', None), # to be set by grid search.\n",
    "])\n",
    "\n",
    "param_grid={\n",
    "    'union__word__processor__process_method': [get_fws_fql, get_tokens_fql],\n",
    "    'selector__k': [100, 'all'],\n",
    "    'clf': [MultinomialNB(), LogisticRegression(solver='liblinear', random_state=0)],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "search = GridSearchCV(model, cv = StratifiedKFold(n_splits=5, random_state=0), \n",
    "                      return_train_score = False, \n",
    "                      scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'],\n",
    "                      refit = 'f1_weighted',\n",
    "                      param_grid = param_grid\n",
    "                     )\n",
    "\n",
    "search.fit(age_X_train, age_y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = search.predict(age_X_test)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(age_y_test, predictions))\n",
    "print(classification_report(age_y_test, predictions))\n",
    "print(confusion_matrix(age_y_test, predictions))\n",
    "\n",
    "confusion_matrix_heatmap(confusion_matrix(age_y_test,predictions), search.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try adding and improving the features used for age classification (or language variant), and evaluate your results with different setups. You can choose to include the larger dataset if you wish, but be aware that running time will increase substantially. There may also be memory errors ([setting `memory` to a temp directory for the Pipeline may help](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline))\n",
    "\n",
    "Marks will be given as follows.\n",
    "- 3 marks will be given for a good range of additional or improved features (> 3), or advanced task (see below).\n",
    "- 2 marks will be given for a completed analysis with some extra features.\n",
    "- 1 mark will be given for a full attempt.\n",
    "This exercise must be demonstrated by Week 19's lab.\n",
    "\n",
    "### Advanced tasks:\n",
    "\n",
    "- You could try predicting the precise age with a regression model (the age is in the json metadata). See: https://dl.acm.org/citation.cfm?id=2107651.\n",
    "- Or you could try predicting age and gender together, either mutli-class pairs, or through a classification tree, e.g. predict gender first, then narrowing age ranges.\n",
    "- Next week we'll be looking at indidual Tweets as \"documents\" or instances. You can try this now: change the json processor to create Documents for each Tweet, you can predict Gender or age based on a single Tweet, or Author (e.g. using Handle as the label). I suggest reducing the number of Authors to 10-20 if trying authorship attribution. With short tweets, it is probably better to binarise count features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
