{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: if running on vagrant VM, make sure you have installed fonts-noto and fonts-emojione:\n",
    "\n",
    "`sudo apt install fonts-noto fonts-emojione`\n",
    "\n",
    "Also make sure packages are installed as usual, with:\n",
    "\n",
    "`sudo pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous weeks we have collected data, and then preprocessed it, including tokenisation to split the text into meaningful units (\"words\"). Given usable text and a token list, next we will look to extract features by counting occurrences of different elements and calculating other features over the text, tokens, and other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Preamble](#preamble)\n",
    "* [Bag of Words](#bow)\n",
    "    - [Filtered List](#filtered)\n",
    "    - [Word N-grams](#wordn)\n",
    "* [Characters](#chars)\n",
    "    - [Char N-grams](#charn)\n",
    "* [Annotation](#ann)\n",
    "* [Other features](#other)\n",
    "* [Documents](#docs)\n",
    "* [Corpus analysis](#corpus)\n",
    "* [TF-IDF](#tfidf)\n",
    "* [Exercise](#ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preamble\"></a>\n",
    "## Preamble\n",
    "We can use code from last week to preprocess our text, a method is defined below to do some basic preprocessing, please check your understanding. You may see fit to edit the preprocessing to suit your needs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "import re\n",
    "\n",
    "hashtag_re = re.compile(r\"#\\w+\")\n",
    "mention_re = re.compile(r\"@\\w+\")\n",
    "url_re = re.compile(r\"(?:https?://)?(?:[-\\w]+\\.)+[a-zA-Z]{2,9}[-\\w/#~:;.?+=&%@~]*\")\n",
    "\n",
    "def preprocess(text):\n",
    "    p_text = hashtag_re.sub(\"[hashtag]\",text)\n",
    "    p_text = mention_re.sub(\"[mention]\",p_text)\n",
    "    p_text = url_re.sub(\"[url]\",p_text)\n",
    "    p_text = ftfy.fix_text(p_text)\n",
    "    return p_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the feature extraction, we're going to start by working with a single tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"This week we‚Äôre at a #careers event in #Blackpool @Pleasure_Beach, talking to students about #languages and language careers! Come have a go at some of our activities! üåè#LoveLanguages #LoveLancaster @Lancaster_CI https://t.co/vQQWdrUuqh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tweet = preprocess(tweet)\n",
    "print(p_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tokenisation, we have a basic custom tokeniser. This is equivalent to the custom tokenisers created last week, but with a pre-compiled regular expression. Alternation is used to separate patterns. Again, you may see fit to edit to suit your needs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenise_re = re.compile(r\"(\\[[^\\]]+\\]|[-'\\w]+|[^\\s\\w\\[']+)\") #([]|words|other non-space)\n",
    "def custom_tokenise(text):\n",
    "    return tokenise_re.findall(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility methods for displaying/saving tokens list. Can be used for any list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens(tokens):\n",
    "    for token in tokens: #iterate tokens and print one per line.\n",
    "        print(token)\n",
    "    print(f\"Total: {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokens(tokens, outfile):\n",
    "    with open(outfile, 'w', encoding=\"utf-8\") as f:\n",
    "        for token in tokens: #iterate tokens and output to file.\n",
    "            f.write(token + '\\n')\n",
    "        f.write(f\"Total: {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bow\"></a>\n",
    "## Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the most common NLP feature set, traditionally, is the \"bag of words\". This is a count of each word in the text, disregarding context. Whilst limited, due to the lack of context, a simple bag of words can achieve reasonable results for simple classification tasks, and is often used as a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to tokenise the text. The tokenisation used will determine what is considered a \"word\", although post processing of the token list could be undertaken, e.g. to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = custom_tokenise(p_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tokens(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple bag of words, it often makes sense to make the token list all lowercase, so the same word with different casings are merged (e.g. if a word is at the beginning of a sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_tokens = [t.lower() for t in tokens] #list comprehension\n",
    "print_tokens(lower_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Python's `lower()` method is Unicode aware, and will lowercase letters with diacritics and from non-Latin alphabets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"√Ö√â√é√ë√áŒõ–§\".lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a frequency list, we simply place the token list in a [`Counter`](https://docs.python.org/3.7/library/collections.html#counter-objects) object, which extends `dict`, mapping items to frequencies. [NLTK's FreqDist](http://www.nltk.org/_modules/nltk/probability.html#FreqDist), which extends `Counter`, could also be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokens_fql = Counter(lower_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_fql.most_common() #displays frequency list in descending frequency order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"filtered\"></a>\n",
    "### Filtered list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At some point we will need to filter the bag of words, e.g. to some top-500 or top-1000 words, as it rarely makes sense to have a feature vector containing all words.\n",
    "\n",
    "The method below uses word frequencies to create a new frequency list containing all in the predefined lists. Including 0s for words not found (dense vector). The vector can be made sparse (remove 0s) by with `+counter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fql(fql, predefined_list):\n",
    "    return Counter({t: fql[t] for t in predefined_list}) #dict comprehension, t: fql[t] is token: freq."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common feature set (especially for authorship analysis) is function words (aka stop words). Here we use the function word list taken from https://ieeexplore.ieee.org/abstract/document/6234420."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_list(file):\n",
    "    with open(file) as f:\n",
    "        items = []\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            items.append(line.strip())\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fws = read_list(\"functionwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fws_fql = filter_fql(tokens_fql, fws)\n",
    "fws_fql.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove 0s, and make into sparse vector: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+fws_fql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note you need to be careful that the tokenisation matches what is in the function word / stopword list.\n",
    "\n",
    "You could also use NLTK's stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stoplist = stopwords.words('english')\n",
    "print(stoplist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove words from a list (e.g. a stopword list) by iterating the list of words to remove, and 'popping off' ([dict.pop(key,None)](https://docs.python.org/3/library/stdtypes.html?highlight=pop#dict.pop)) each word if present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_list(fql, to_remove):\n",
    "    filtered = Counter(fql)\n",
    "    for r in to_remove:\n",
    "        filtered.pop(r,None)        \n",
    "    return filtered\n",
    "\n",
    "filtered = remove_list(tokens_fql, stoplist)\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordn\"></a>\n",
    "### Word n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some context for words, we can use sequences of words instead of single words, these are known as word n-grams. bigrams (2-grams) and trigrams (3-grams) are popular. One issue with word n-grams is their sparsity. It's a good idea to reduce the size of the vocabulary as much as possible, e.g. digits and dates could be mapped to single tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether a token appears at the start or end of the text (or could be sentence) can be useful, so we can introduce buffer markers at the start and end to indicate this.\n",
    "\n",
    "Note also that n-grams should be created with a sliding window over the text, i.e. the first word bigram is the first and second word, the second bigram is the second and third word.\n",
    "\n",
    "The method below is a generic method for turning a list of tokens into an n-gram list, adding the buffer characters either side, and moving a sliding window of size n across the text and providing a list of n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(tokens, n, sep = \"_\", buffer=\"^\"):\n",
    "    buffered = [buffer] * (n-1) + tokens + [buffer] * (n-1) #add buffer either side to denote start and end\n",
    "    return [sep.join(buffered[i:i+n]) for i in range(len(buffered)-n+1)] #list comprehension creating merged string of n chars, with a window of n through string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_bigrams = ngrams(lower_tokens,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_bigrams_fql = Counter(word_bigrams)\n",
    "word_bigrams_fql.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Produce a frequency list of word trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"chars\"></a>\n",
    "## Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just looking at characters as features is a simple (yet often powerful) way of processing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably don't want the artificial hashtag, mention, and url markers, we could keep these as is, replace with single chars, or just remove them. Below we just remove them. We often have different pre-processing for different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_remove(text):\n",
    "    r_text = hashtag_re.sub(\"\",text)\n",
    "    r_text = mention_re.sub(\"\",r_text)\n",
    "    r_text = url_re.sub(\"\",r_text)\n",
    "    r_text = ftfy.fix_text(r_text)\n",
    "    return r_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_tweet = preprocess_remove(tweet)\n",
    "print(r_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, extra spaces are included now, how could you preprocess the text further to reduce multiple spaces to a single space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python a string is just a sequence (list) of characters, so we can just iterate through the characters as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for char in r_tweet:\n",
    "    print(char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make thus count the frequency of each character easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_fql = Counter(r_tweet)\n",
    "char_fql.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appears to work well, **but this should be used with caution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Remember the spicy jalapen\\u0303o\"\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, char in enumerate(test):\n",
    "    print(i,char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the  ÃÉ separated from the n because it is a separate codepoint (combining)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets worse if we view the characters as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the  ÃÉ over the single quote mark. Nasty! ü§¢\n",
    "\n",
    "The combining codepoint combines with whatever the character before is, and in this case it's displayed as the quote mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be careful how we define \"character\". In Python a 'character' is a single Unicode codepoint. When in reality, we should be looking for \"graphemes\", i.e. displayed single characters (which may be a cluster of codepoints): https://unicode.org/reports/tr29/#Grapheme_Cluster_Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use regular expressions to find these graphemes, but Python's default regular expression library (re), whilst being Unicode aware, does not deal with Unicode particularly well. The [regex library](https://pypi.org/project/regex/) has better support, providing the use of unicode categories: https://www.regular-expressions.info/unicode.html, including `\\X` to match single graphemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "char_regex = regex.compile(r'\\X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = char_regex.findall(test)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This nicely separates √± as single \"character\". We can put this into a frequency list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_fql = Counter(chars)\n",
    "char_fql.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more \"fun\" can be had with emojis, which can contain numerous codepoints, particularly joined with zero-width-joiners: https://unicode.org/emoji/charts/emoji-zwj-sequences.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_test = \"This is an emoji: \\U0001F468\\u200D\\U0001F469\\u200D\\U0001F467\\u200D\\U0001F466\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emoji_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matches = char_regex.findall(emoji_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in test_matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You must have the latest version (2019.2.18) of the regex library for this to work, there was a bug I actually found whilst preparing this lab, which has now been fixed: https://bitbucket.org/mrabarnett/mrab-regex/issues/312/x-not-matching-graphemes-with-zero-width.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another library, [grapheme](https://pypi.org/project/grapheme/), also provides funtionality to deal with these graphemes like characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grapheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphemes = list(grapheme.graphemes(emoji_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in graphemes:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_fql = Counter(graphemes)\n",
    "char_fql.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, when composite graphemes are printed in a list/tuple, they're expanded for some reason (if you know why, please tell me!). As can be seen, this is just a display issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for char in char_fql.most_common():\n",
    "    print(\"{}\\t{}\".format(char[0], char[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"charn\"></a>\n",
    "### Character n-grams\n",
    "\n",
    "We can also look at sequences of characters, though be aware that these will overlap with words and other features (double counting).\n",
    "\n",
    "You have everything you need to do this (remember the n-grams function is generic).\n",
    "\n",
    "**Task:** Produce character trigrams for the tweet. You don't need a separator for chars, so the first trigram should be '^^T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ann\"></a>\n",
    "## Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the lecture, various levels of annotation are available to add on top of the tokens. These are extra levels of information that can be used as features for various NLP tasks. Lemmatisation is one option available and straightforward to [implement with nltk](http://www.nltk.org/book/ch03.html#lemmatization).\n",
    "\n",
    "Part-of-speech (POS) tags are probably the most used form of annotation, certainly for classification tasks. NLTK provides a POS tagger using the standard [Penn Treebank POS tagset](https://www.anc.org/penn.html). Tokenised text can be POS tagged easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "pos_tagged = nltk.pos_tag(tokens)\n",
    "pos_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of the accuracy of the POS tags on this small sample? You can see a description of each POS tag with the below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add notes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"tagsets\")\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a POS frequency list is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [tag[1] for tag in pos_tagged]\n",
    "pos_fql = Counter(pos)\n",
    "pos_fql.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Try to make improvements to the POS tagging by changing the preprocessing and tokenisation. As a minimum, try using NLTK's default tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced task:** \n",
    "\n",
    "Developing a POS tagger that is capable of dealing well with the intricacies of user generated content (e.g. Twitter) text is difficult, although there have been attempts, e.g. http://www.cs.cmu.edu/~ark/TweetNLP/. One option is to post-process the POS tagged text to fix the main issues.\n",
    "\n",
    "Define a function that takes the POS tagged text and post-processes the output to add new tags for mentions, hashtags, urls, emojis, and anything else you can see to fix with simple rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"other\"></a>\n",
    "## Other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many other features can be calculated over the text, token stream, or other feature frequency lists. Some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_chars = len(tweet) #length of text in chars\n",
    "length_tokens = len(tokens) #length of text in tokens\n",
    "print(length_chars)\n",
    "print(length_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average word length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_word_length = sum([len(tok) for tok in tokens])/length_tokens #make a list of lengths per token, sum and divide by number of tokens\n",
    "print(avg_word_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various vocabulary measures are available that represent how varied and large the vocabulary is of the text.\n",
    "\n",
    "We need to know the number of **word types** are present, this is the number of words, counting multiple instances (tokens) of the same word once. This is simply the size of the frequency list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_types = len(tokens_fql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type Token Ratio (TTR) is a popular vocabulary measure, simply dividing the number of types by the number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr = length_types / length_tokens #type token ratio (ttr)\n",
    "print(ttr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TTR is not comparable over texts of very different lengths, instead use something like Moving-Average Type-Token Ratio (MATTR): https://doi.org/10.1080/09296171003643098\n",
    "\n",
    "**Advanced task**: Reading the above linked paper, implement MATTR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other vocabulary measures look at the number of hapaxes (words types which only appear once), below a simple hapax ratio is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hapaxes = list(tokens_fql.values()).count(1) #convert frequencies to list and count 1s.\n",
    "hapax_ratio = hapaxes / length_types\n",
    "print(hapax_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other features that could be implemented. Readability metrics could be calculated, most of which require a count of syllables. Counting syllables is actually [quite an involved task](https://stackoverflow.com/questions/405161/detecting-syllables-in-a-word/4103234), especially for user generated content, and multi-lingual data. [Big Phoney](https://github.com/repp/big-phoney) is one option that seems promising (based on some limited testing). An **Advanced Task** would be to implement one or more the readability measures (e.g. [*Flesch reading ease*](https://en.wikipedia.org/wiki/Flesch‚ÄìKincaid_readability_tests))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting and splitting text into sentences is also needed for some features. This is quite simple to do with NLTK, as below. Though be aware, like other segmentation tasks, doing this accurately with user generated content is not straight-forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(p_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"docs\"></a>\n",
    "## Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been utilising a single line of text (Tweet) to demonstrate feature extraction. However, we will normally be dealing with larger texts consisting of lines of texts (e.g. paragraphs or Tweets), we can call these documents. We normally do not want sequence features (e.g. n-grams) to go across line boundaries within a document. Hence we process and extract features per line of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things a little easier, we create a `Document` class which holds the features of a document (and any metadata provided). Features are calculated with the `extract_featues` function, which takes in a list (iterable) of texts (which could be lines in a text, or individual tweets from a user). Currently, just tokens are counted (i.e. bow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, meta={}):\n",
    "        self.meta = meta\n",
    "        self.tokens_fql = Counter() #empty counter, ready to be added to with Counter.update.\n",
    "        \n",
    "    def extract_features(self, texts): #document should be iterable text lines, e.g. read in from file.\n",
    "        for text in texts:\n",
    "            p_text = preprocess(text)\n",
    "            tokens = custom_tokenise(p_text.lower())\n",
    "            self.tokens_fql.update(tokens) #updating Counter counts items in list, adding to existing Counter items.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilise this, we simply create a Document, and add text to it. An example using the existing tweet we've been using is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_doc = Document()\n",
    "tweet_doc.extract_features([tweet])\n",
    "print(tweet_doc.tokens_fql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mps\"></a>\n",
    "## MPs Dataset\n",
    "In order to play with features a collection of Tweets from MP accounts is provided in the [mps folder](mps). These are plain text files for each user, split into Labour and Conservative. (Some MPs have [actually left the Labour party](https://www.bbc.co.uk/news/topics/ckdkngd45nlt/labour-party-resignations), could be interesting cases for study).\n",
    "\n",
    "The corpus can be read into Documents as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, splitext, split\n",
    "\n",
    "\n",
    "def import_party_folder(party):\n",
    "    folder = \"mps/\" + party\n",
    "    textfiles = [join(folder, f) for f in listdir(folder) if isfile(join(folder, f)) and f.endswith(\".txt\")]\n",
    "    for tf in textfiles:\n",
    "        username = splitext(split(tf)[1])[0] #extract just username from filename.\n",
    "        print(\"Processing \" + username)\n",
    "        doc = Document({'username': username, 'party': party}) #include metadata\n",
    "        with open(tf) as f:\n",
    "            tweets = f.readlines()\n",
    "        doc.extract_features(tweets)\n",
    "        yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "corpus.extend(import_party_folder(\"labour\"))\n",
    "corpus.extend(import_party_folder(\"conservative\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a **corpus** of MPs on Twitter we can use for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in corpus:\n",
    "    print(doc.meta['username'], doc.meta['party'], sum(doc.tokens_fql.values()),sep=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corpus\"></a>\n",
    "## Corpus analysis\n",
    "We can compare corpora or sub-corpora to start to gain insights into language differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our frequency lists (FQLs) are stored as [`Counters`](https://docs.python.org/3.7/library/collections.html#counter-objects), which can be merged easily by just adding them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_fqls(docs):\n",
    "    merged = Counter()\n",
    "    for doc in docs:\n",
    "        merged += doc.tokens_fql\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sub-corpus, one for Conservative MPs, another for Labour MPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_fql = merge_fqls([doc for doc in corpus if doc.meta['party']==\"conservative\"])\n",
    "lab_fql = merge_fqls([doc for doc in corpus if doc.meta['party']==\"labour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_size = sum(con_fql.values())\n",
    "lab_size = sum(lab_fql.values())\n",
    "print(con_size,lab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start analysing the most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lab_fql.most_common(20))\n",
    "print(con_fql.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And even create a basic [word cloud](https://github.com/amueller/word_cloud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_wordcloud(words):\n",
    "    wordcloud = WordCloud().generate_from_frequencies(words)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(con_fql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(lab_fql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not that interesting as common words dominate. How could we remove these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalise the frequencies, we can simply divide by the number of tokens, to gain relative frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_freqs(fql):\n",
    "    size = sum(fql.values())\n",
    "    return {term: fql[term]/size for term in fql}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_rel = relative_freqs(con_fql)\n",
    "lab_rel = relative_freqs(lab_fql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do a \"Key words\" comparison between the sub-corpora, we can utilise [*Log Ratio*](http://cass.lancs.ac.uk/log-ratio-an-informal-introduction/), which is the binary log of the relative risk (ratio between relative frequencies). Other significance tests and effect size measures can be used: http://ucrel.lancs.ac.uk/llwizard.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "#Calculates log ratio for terms in corpus1, compared to corpus2.\n",
    "#we pass the corpus sizes for ease.\n",
    "#If the term is not present in corpus2, we make the frequency 0.5.\n",
    "def log_ratio(corpus1, corpus1_size, corpus2, corpus2_size):\n",
    "    return {term: log((corpus1[term]/corpus1_size)/((corpus2[term] if corpus2[term] else 0.5)/corpus2_size),2) for term in corpus1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the terms from Conservative MPs with the biggest log ratio compared to terms from Labour MPs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_lr = log_ratio(con_fql, con_size, lab_fql, lab_size)\n",
    "sorted_terms = sorted(con_lr.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_terms[:20])\n",
    "create_wordcloud(con_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the other way round:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_lr = log_ratio(lab_fql, lab_size, con_fql, con_size)\n",
    "sorted_terms = sorted(lab_lr.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_terms[:20])\n",
    "create_wordcloud(lab_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some interesting terms appear, but with a small number of authors, some terms will be prominent from one MP, boosting the frequency in the sub-corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tfidf\"></a>\n",
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the lecture, TF-IDF is a commonly used normalisation method which considers the term frequency along with how many documents in the corpus the term appears in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc is a Counter representing an fql from a document.\n",
    "def tf(term, doc):\n",
    "    return doc[term] / sum(doc.values()) #term freq / total terms (relative term freq)\n",
    "\n",
    "def num_containing(term, corpus):\n",
    "    return sum(1 for doc in corpus if term in doc) #counts docs in corpus containing term.\n",
    "\n",
    "#1 added to numerator and denominator is for preventing division by zero. Equivalent of an extra document containing all terms once.\n",
    "def idf(term, corpus):\n",
    "    n_t = num_containing(term,corpus)\n",
    "    return log((len(corpus)+1) / ((n_t) + 1))\n",
    "    \n",
    "def tfidf(term, doc, corpus):\n",
    "    return tf(term, doc) * idf(term, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the TF-IDF for every term for every MP in the corpus. By listing the terms with the highest TF-IDF, we can look at terms that are used by that MP frequently, but only used by that MP alone, or a small number of MPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_fqls = [doc.tokens_fql for doc in corpus]\n",
    "for doc in corpus:\n",
    "    print(doc.meta['username'], doc.meta['party'])\n",
    "    scores = {term: tfidf(term,doc.tokens_fql,corpus_fqls) for term in doc.tokens_fql}\n",
    "    sorted_terms = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for term, score in sorted_terms[:5]:\n",
    "        print(\"\\tToken: {}, TF-IDF: {}\".format(term, round(score, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ex\"></a>\n",
    "## Exercise\n",
    "Use the MPs data you already have to conduct some further feature extraction and analysis. Please write code to answer the following questions:\n",
    "\n",
    "1. Which MP in the dataset has the highest average word length?\n",
    "2. What are the 5 **key part-of-speech tags** used by Labour MPs more than Conservative MPs?\n",
    "3. **Advanced:** What hashtags does Jeremy Corbyn use frequetly, which aren't used widely by the rest of the Labour party MPs in the dataset (TF-IDF)?\n",
    "4. **Advanced:** If you want to go further, devise your own research question, either using the MP data provided, [collecting more MPs](mp_accounts), or on different data.\n",
    "\n",
    "You may need to pre-process and tokenise the text differently. I suggest re-using the code above, including adapting the Document class, adding/editing preprocessing, tokenisation, and feature extraction.\n",
    "\n",
    "If you prefer, you can create a new notebook for the exercise work. The methods and imports above are provided in a Python file too: [features.py](features.py), to read this in use `%run features.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for 1 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for 2 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for 3 here (advanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for 4 here (advanced)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. Question: Answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
